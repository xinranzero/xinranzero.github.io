[{"title":"【系统设计】崩溃一致性","path":"/2026/01/06/【系统设计】崩溃一致性/","content":"什么是崩溃一致性？设计背景在涉及状态持久化的系统中（如数据库、元数据服务、资产系统、任务状态系统等），写操作不可避免地面临以下风险： 进程异常退出 主机宕机或掉电 磁盘写入中断 写入顺序被打乱 如果缺乏专门设计，系统可能在崩溃后处于一种历史上从未合法存在过的中间状态，导致： 数据损坏不可解析 业务状态不一致（如部分更新生效） 无法自动恢复，只能人工介入 因此，需要一种崩溃一致性（Crash Consistency）保障机制，确保系统在异常情况下仍能恢复到一个合法、可解释的状态。 👉 Crash Consistency（崩溃一致性）：系统在“写入过程中崩溃（掉电 &#x2F; 宕机）时，如何保证数据不损坏、不自相矛盾” 常见场景 钱扣了，余额没加 索引更新了，数据没写 指针改了，数据还没落盘 这在掉电时是最容易发生的。 设计目标本方案的设计目标如下： 防止写入中间态被持久化 支持系统崩溃后的自动恢复 尽量降低对写性能的影响 方案可工程化落地，复杂度可控 3类崩溃问题概述从底层视角看，存储系统在崩溃时主要面临三类问题： 部分写入（Partial Write）数据页只写入了一半，导致数据结构损坏。 写入顺序失序（Reordering）实际落盘顺序与程序逻辑顺序不一致。 原子性破坏（Atomicity Violation）一个逻辑操作仅部分生效。 ① 部分写入（Partial Write） 一页 &#x2F; 一个 block 只写成功了一半 磁盘页被撕裂 校验失败 数据不可解析 方案 是否解决 WAL ✅ 双写 ✅（主要就是干这个） CoW ✅ ② Reordering（写顺序被打乱） 你以为是 A → B实际落盘顺序是 B → A 方案 是否解决 WAL ✅（顺序日志 + fsync） 双写 ❌ CoW ✅（指针原子切换） ③ Atomicity（原子性破坏） 一个操作要么全成，要么全不成 方案 是否解决 WAL ✅ 双写 ❌ CoW ✅（天然） 3种解决方案Write-Ahead Logging（WAL）方案说明WAL 的核心思想是： 在修改数据之前，先将“将要发生的操作”可靠记录下来。 通过保证日志先于数据落盘，即使在任意时刻发生崩溃，也可以基于日志对系统状态进行回放或回滚。 工作流程12341. 将操作追加写入日志（顺序写）2. fsync 日志，确保持久化3. 修改数据页（随机写）4. 后台 checkpoint / 刷盘 崩溃恢复行为 日志未落盘：操作视为未发生 日志已落盘、数据未完成：通过 redo 恢复 日志与数据均完成：状态一致 优点 能完整表达事务语义 掉电恢复逻辑清晰 顺序写为主，性能较优 工程成熟度高 缺点 实现复杂度较高 需要日志管理与回收机制 适用场景 数据库事务系统 消息队列 元数据 &#x2F; 状态型服务 双写（Double Write）方案说明双写机制通过冗余写入来防止单次写入失败导致的数据页损坏。 其核心假设是： 一次写入不可靠，但两次写入至少能保留一份完整数据。 典型流程（以 MySQL 为例）1231. 数据页写入 doublewrite buffer2. fsync3. 再写回原始数据页位置 崩溃恢复行为 若数据页损坏，可使用 doublewrite buffer 中的完整页进行修复 优点 有效解决页级部分写问题 对现有存储结构侵入较小 局限性 不具备事务语义 无法描述“操作是否完成” 不支持 redo &#x2F; undo 适用场景 作为 WAL 的补充机制 解决页级物理损坏问题 Copy-on-Write（CoW）方案说明CoW 通过写新版本、保留旧版本的方式，彻底避免覆盖写入，从设计上杜绝中间态。 写入流程1231. 新数据写入新位置2. fsync 新数据3. 原子性更新根指针 / 元数据 崩溃恢复行为 指针未更新：仍指向旧版本 指针更新完成：新版本生效 优点 天然具备崩溃一致性 不需要 redo &#x2F; undo 缺点 写放大严重 数据碎片化 实现复杂，对内存与结构设计要求高 适用场景 文件系统 不可变数据结构 快照优先的存储系统 方案对比与选型依据 维度 WAL 双写 CoW 解决层级 事务 &#x2F; 语义 页完整性 结构一致性 掉电一致性 强 中 强 事务支持 完整 无 需额外设计 写性能 高 中 低 工程复杂度 中 低 高 典型使用 MySQL InnoDB、PostgreSQL，MongoDB的WiredTiger InnoDB doublewrite ZFS、LMDB WAL 是事务型系统中最通用、最成熟的一致性方案，在“性能 &#x2F; 语义 &#x2F; 复杂度 &#x2F; 可扩展性”上是最均衡解，可以用工程手段控制复杂度 双写适合作为底层页安全补丁，治标不治本 CoW 设计太重，不适合高更新 OLTP，更适用于从架构层设计的一致性系统 如果你在 自己设计系统： 要事务 + 高写入 → WAL 只是怕页损坏 → 双写 不可变数据 &#x2F; 快照优先 → CoW 配置 &#x2F; 元数据系统 → WAL 或 CoW WAL思想的使用场景WAL（Write-Ahead Logging）是一种用于保证系统在崩溃或掉电情况下仍能恢复到一致状态的通用设计思想。它并不局限于数据库，而是一种适用于所有“需要持久化状态、且不能接受中间态”的系统的基础机制。WAL 的核心原则只有一句话：在修改任何持久化数据之前，必须先把“将要发生的操作”可靠地记录下来。 只要这条日志已经落盘，即使系统随后发生崩溃，也可以通过重放或回滚日志，把系统恢复到一个合法状态。 WAL（Write-Ahead Logging）的思想，凡是满足下面三点之一，就值得用： 1️⃣ 有“写状态”2️⃣ 不能接受中间态 &#x2F; 数据损坏3️⃣ 需要崩溃后自动恢复 满足任意两点，WAL 基本就是最优解。 从工程角度看，WAL 之所以成立，是因为日志比真实数据更容易被可靠写入。 日志通常是顺序追加、数据量小、结构简单，可以高效地 fsync 到磁盘；而真实数据往往是随机写入、体积大、结构复杂，更容易在掉电时出现页撕裂或写入顺序错乱。WAL 的设计并不试图让“数据写入本身变得绝对安全”，而是通过先保证日志的安全性，把恢复的可能性提前锁死。只要系统存在不可丢失的状态，并且不能接受写到一半的结果，那么 WAL 就应该被认真考虑；而一旦系统本身不关心崩溃恢复，WAL 就不再是必需品。 WAL 在哪里是“必选项”（已经内建）1️⃣ 数据库（你最熟的）在数据库系统中，WAL 是存储引擎层面的基础机制，而不是一种可选优化。无论是 MySQL InnoDB、PostgreSQL 还是 MongoDB 的 WiredTiger，引擎都会强制要求：事务提交之前，相关的 redo 日志必须先落盘。数据库并不要求数据页在提交时已经写入磁盘，只要日志是完整的，就可以在重启时通过日志重放恢复数据。这意味着，在数据库语境下，WAL 是事务原子性和持久性的根基，而不是开发者“建议采用”的最佳实践。 MySQL的redo log详解：MySQL三大日志(binlog、redo log和undo log)详解 | JavaGuide MySQL &#x2F; PostgreSQL &#x2F; Oracle MongoDB（WiredTiger） SQLite 作用： 事务原子性 掉电恢复 崩溃一致性 👉 在这里：WAL &#x3D; 存储引擎的一部分，不可绕过 2️⃣ 消息队列 &#x2F; 日志系统在消息队列和日志系统中，WAL 以另一种形式存在，但解决的是同一类问题。以 Kafka 为例，消息首先被顺序追加到 commit log 中，这个 commit log 本质上就是一条 WAL。只要消息已经写入日志文件并持久化，即使 Broker 崩溃，消息也不会丢失。消费者的位点、生产者的确认语义，都是围绕这条日志构建的。这里的 WAL 不一定叫 “redo log”，但它承担的职责完全一致：先保证事实被记录，再考虑派生状态。 Kafka Pulsar RocketMQ WAL 的形态： 顺序 append 的 commit log 解决的问题： 消息不丢 顺序可恢复 at-least-once &#x2F; exactly-once 👉 Kafka 本质上就是一个 WAL-first 系统 WAL 在哪里是“最佳实践”（但你要自己实现）3️⃣ 自研存储 &#x2F; KV &#x2F; 状态服务在自研的状态型系统中，WAL 通常以“操作日志”或“事件日志”的形式出现。例如配置中心、元数据服务、资产关系系统、任务调度系统等，只要系统内部存在“状态变更”，并且希望在进程崩溃后自动恢复，就可以引入 WAL 思想：先把状态变更写入日志，再更新内存状态或当前视图，最后通过快照或异步刷盘固化结果。这类系统往往并不需要完整的数据库级事务，但 WAL 能以较低成本提供足够强的崩溃一致性保障。 典型场景： 配置中心 资产系统（你之前做的那种） 小型元数据服务 用法： 123append log → fsyncapply to memoryasync flush snapshot 解决： 掉电 进程 crash 滚动升级 4️⃣ 分布式一致性组件在分布式一致性系统中，WAL 是共识算法落地的基础载体。以 Raft 为例，每个节点都会先把日志条目追加到本地磁盘，再参与提交与状态机应用。Raft 本身解决的是“多副本一致性”问题，而 WAL 解决的是“单节点崩溃后如何恢复状态”的问题。两者关注的层级不同，但 WAL 是共识系统能够正确工作的前提条件。 etcd &#x2F; Zookeeper &#x2F; Consul WAL 的角色： Raft log 状态机 replay 👉 共识 ≠ WAL，但 WAL 是共识的落盘载体 5️⃣ 文件系统 &#x2F; 存储引擎 ext4 &#x2F; XFS（journal） 自研对象存储元数据 解决： inode &#x2F; 元数据一致性 文件系统修复时间 WAL 在业务系统里的“隐形用法”（很多人忽略）6️⃣ 任务 &#x2F; 工作流系统比如： 定时任务 状态机流转 审批流程 用 WAL 思想你会这样做： 状态变更先写事件表 再更新当前状态 👉 Event Sourcing &#x3D; WAL 思想的高级形态 7️⃣ 异步系统 &#x2F; 补偿系统 订单创建 资金流水 资产变更 WAL 化表现： 操作日志表 outbox 表 binlog 什么时候不适合用 WAL？需要明确的是，WAL 并不是在任何场景下都值得使用。如果系统是纯缓存型、允许数据丢失、状态可以通过上游重算，或者所有写入都是全量覆盖式的，那么引入 WAL 只会增加复杂度而没有实际收益。WAL 的价值只在于：系统真的在乎“崩溃之后还能不能自动回到正确状态”。 ❌ 不适合 WAL 的情况： 纯缓存（Redis cache-only） 可丢数据（metrics、trace） 只读系统 全量覆盖写（例如每天重算） fsync 的原理fsync 的核心作用是确保操作系统缓存中的数据被真正写入到物理存储介质上。在现代操作系统中，文件写入通常是先写入内存页缓存（page cache），而不是直接写入磁盘。这样做是为了性能，因为内存读写速度远快于磁盘。当程序调用 write() 或者文件写操作时，数据通常先留在 page cache 中，这意味着即使 write() 返回成功，数据也可能还没落到磁盘上。如果此时系统突然掉电或者宕机，这些数据可能丢失。fsync 正是为了解决这个问题而存在的。它的作用是告诉操作系统：请把指定文件的所有缓存数据，以及文件的元数据（如文件大小、修改时间、inode 信息）同步写入磁盘，并等到写入完成后再返回给用户进程。 换句话说，fsync 是一种强制持久化保证，确保在调用返回时，数据已经安全落盘。 从底层原理来看，fsync 涉及以下几个步骤： 刷新页缓存（Dirty Pages）：操作系统会扫描指定文件的 page cache，把所有“脏页”（已修改但未写入磁盘的页面）排队进行写入。 下发写命令到存储设备：操作系统通过块设备接口（block device）把这些页发送给磁盘或者 SSD。 等待写入完成：操作系统不会立即返回，而是等待存储设备确认所有数据已经物理写入。 对传统机械硬盘，确认意味着磁头已写入磁道。 对 SSD，确认意味着控制器已完成闪存写入并更新持久映射表。 同步元数据：为了保证文件结构完整性，fsync 还会写入 inode、目录信息、分配表等元数据。 需要注意的是，fsync 的保证依赖两个条件：操作系统和存储设备都必须支持强一致性。大多数现代操作系统（Linux、Windows、MacOS）提供了可靠的 fsync，但部分廉价 SSD 或者虚拟化环境可能使用缓存策略，掉电时仍有数据丢失的风险。在关键系统中，还会要求开启硬件写入缓存刷盘（flush on power loss）或者使用电容缓存的存储阵列来保证掉电安全。 在数据库或 WAL 场景下，fsync 是实现 Durability（持久性） 的关键操作。WAL 的日志先写入 page cache，然后通过 fsync 强制落盘，保证事务提交的操作在系统崩溃后依然可恢复。没有 fsync，即使日志写入了内存，也可能因为掉电而丢失，破坏事务持久性。 拓展阅读 MySQL三大日志(binlog、redo log和undo log)详解 | JavaGuide WiredTiger: WiredTiger Architecture Guide https://share.google/aimode/VysnWHf1gVSJGZFNC"},{"title":"【消息队列】基本概念与设计理念","path":"/2025/12/27/【消息队列】基本概念与设计理念/","content":"消息队列1. 消息队列是什么？如何选型？ 定义的广义化： 消息队列本质上是具有 “缓冲作用” 和 “发布&#x2F;订阅能力” 的存储引擎。 技术的发展由 需求 和 底层架构 双重驱动： 需求面（功能演进）：消息 (异步&#x2F;解耦) –&gt; 流 (高吞吐&#x2F;削峰) –&gt; 消息和流融合 (一站式解决) 架构面（技术演进）：单机 –&gt; 分布式 (分区&#x2F;副本) –&gt; 云原生&#x2F;Serverless (计算存储分离&#x2F;按量付费) 选型标准： 取决于 数据量 和 业务复杂度。 非标准方案： 如果数据结构简单、流量不大，用 Redis (List&#x2F;PubSub) 或 MySQL 模拟消息队列更省运维成本。 标准方案： 只有在面临 大消息、高并发、持久化堆积 等复杂需求时，才必须引入专门的消息队列产品。 追求大数据下的绝对稳定，选 Kafka；如果你是国内微服务业务架构，选 RocketMQ；如果你想尝试最新的云原生架构并解决扩容痛点，可以关注 Pulsar。 2. 消息队列的基本特性与基本概念基本特性基本特性：高性能、高吞吐、低延时。 常见的使用场景： 一份数据需要被多个下游系统处理，单流程是一个典型的 系统解耦、 消息分发 的场景。 日志采集流程，一般日志数据都很大，直接发到下游，下游系统可能会扛不住崩溃，所以会把数据先缓存到消息队列中。 架构层面的基本概念 Broker ： Broker 本质上是一个进程，比如 RocketMQ 的 Broker 就是指RocketMQ Server 启动成功后的一个进程。在实际部署过程中，通常一个物理节点只会起一个进程，所以大部分情况下我们认为 Broker 就表示一个节点，但是在一些特殊场景下，一个物理节点中也可以起多个进程，就表示一台节点有多个Broker。 Topic（主题） ： 在大部分消息队列中，Topic 都是指用来组织分区关系的一个逻辑概念。通常情况下，一个 Topic 会包含多个分区。但是 RabbitMQ 是一个例外，Topic 是指具体某一种主题模式。 Partition&#x2F;Queue&#x2F;MessageQueue（分区&#x2F;分片）： 在消息队列中，分区、分片、Partiton、Queue、MessageQueue 是一个概念，后面统一用分区来称呼，都是用来表示数据存储的最小单位。一般可以直接将消息写入到一个分区中，也可以将消息写入到Topic，再分发到具体某个分区。一个Topic 通常会包含一个或多个分区。 Producer（生产者）： 生产者指消息的发送方，即发送消息的客户端，也叫生产端。 Consumer（消费者）：消费者指消息的接收方，即接收消息的客户端，也叫消费端。 ConsumerGroup&#x2F;Subscription（消费分组&#x2F;订阅）：一般情况下，消息队列中消费分组和订阅是同一个概念，后面统一用消费分组来称呼。它是用来组织消费者和分区关系的逻辑概念，也有保存消费进度的作用。 Message（消息）：指一条真实的业务数据，消息队列的每条数据一般都叫做一条消息。 Offset&#x2F;ConsumerOffset&#x2F;Cursor（位点&#x2F;消费位点&#x2F;游标）： 指消费者消费分区的进度，即每个消费者都会去消费分区，为了避免重复消费进度，都会保存消费者消费分区的进度信息。 ACK&#x2F;OffsetCommit（确认&#x2F;位点提交）：确认和位点提交一般都是指提交消费进度的操作，即数据消费成功后，提交当前的消费位点，确保不重复消费。 Leader&#x2F;Follower（领导者&#x2F;追随者，主副本&#x2F;从副本）：Leader 和 Follower一般是分区维度副本的概念，即集群中的分区一般会有多个副本。此时就会有主从副本的概念，一般是一个主副本配上一个或多个从副本。 Segment（段&#x2F;数据分段）：段是指消息数据在底层具体存储时，分为多个文件存储时的文件，这个文件就叫做分区的数据段。即比如每超过 1G 的文件就新起一个文件来存储，这个文件就是Segment。基本所有的消息队列都有段的概念，比如Kakfa的Segment、Pulsar的Ledger等等。 StartOffset&#x2F;EndOffset（起始位点&#x2F;结束位点）：起始位点和结束位点是分区维度的概念。即数据是顺序写入到分区的，一般从0的位置开始往后写，此时起始位点就是0。因为数据有过期的概念，分区维度较早的数据会被清理。此时起始位点就会往后移，表示当前阶段最早那条有效消息的位点。结束位点是指最新的那条数据的写入位置。因为数据一直在写入分区，所以起始位点和结束位点是一直动态变化的。 ACL（访问控制技术）：ACL 全称是Access Control List，用来对集群中的资源进行权限控制，比如控制分区或Topic的读和写等。 功能层面的基本概念相比于数据库的基本操作是增删改查，消息队列的基本操作就是生产和消费，即读和写。消息队列一般是不支持客户端修改和删除单条数据的。接下来我们就从功能的角度，来了解一些常见的基本概念。 顺序消息： 是指从生产者和消费者的视角来看，生产者按顺序写入Topic的消息，在消费者这边能不能按生产者写入的顺序消费到消息，如果能就是顺序消息。 延时消息&#x2F;定时消息：都是指生产者发送消息到 Broker 时，可以设置这条消息在多久后能被消费到，当时间到了后，消息就会被消费到。延时的意思就是指以 Broker 收到消息的时间为准，多久后消息能被消费者消费，比如消息发送成功后的30分钟才能被消费。定时是指可以指定消息在设置的时间才能被看到，比如设置明天的20:00才能被消费。从技术上来看，两者是一样的；从客户端的角度，功能上稍微有细微的差别；从内核的角度，一般两种消息是以同一个概念出现的。 事务消息：消息队列的事务因为在不同的消息队列中的实现方式不一样，所以定义也不太一样。正常情况下，事务表示多个操作的原子性，即一批操作要么一起成功，要么一起失败。在消息队列中，一般指发送一批消息，要么同时成功，要么同时失败。 消息重试：消息重试分为生产者重试和消费者重试。生产者重试是指当消息发送失败后，可以设置重试逻辑，比如重试几次、多久后重试、重试间隔多少。消费者重试是指当消费的消息处理失败后，会自动重试消费消息。 消息回溯：是指当允许消息被多次消费，即某条消息消费成功后，这条消息不会被删除，还能再重复到这条消息。 广播消费：广播听起来是一个主动的，即 Broker 将一条消息广播发送给多个消费者。但是在消息队列中，广播本质上是指一条消息能不能被很多个消费者消费到。只要能被多个消费者消费到，就能起到广播消费的效果，就可以叫做广播消费。 死信队列：死信队列是一个功能，不是一个像分区一样的实体概念。它是指当某条消息无法处理成功时，则把这条消息写入到死信队列，将这条消息保存起来，从而可以处理后续的消息的功能。大部分情况下，死信队列在消费端使用得比较多，即消费到的消息无法处理成功，则将数据先保存到死信队列，然后可以继续处理其他消息。当然，在生产的时候也会有死信队列的概念，即某条消息无法写入Topic，则可以先写入到死信队列。从功能上来看，死信队列的功能业务也可以自己去实现。消息队列中死信队列的意思是，消息队列的SDK已经集成了这部分功能，从而让业务使用起来就很简单。 优先级队列：优先级队列是指可以给在一个分区或队列中的消息设置权重，权重大的消息能够被优先消费到。大部分情况下，消息队列的消息处理是FIFO先进先出的规则。此时如果某些消息需要被优先处理，基于这个规则就无法实现。所以就有了优先级队列的概念，优先级是消息维度设置的。 消息过滤：是指可以给每条消息打上标签，在消费的时候可以根据标签信息去消费消息。可以理解为一个简单的查询消息的功能，即通过标签去查询过滤消息。消息过滤主要在消费端生效。 消息过期&#x2F;删除（TTL）：是指消息队列中的消息会在一定时间或者超过一定大小后会被删除。因为消息队列主要是缓冲作用，所以一般会要求消息在一定的策略后会自动被清理。 消息轨迹：是指记录一条消息从生产端发送、服务端保存、消费端消费的全生命周期的流程信息。用来追溯消息什么时候被发送、是否发送成功、什么时候发送成功、服务端是否保存成功、什么时候保存成功、被哪些消费者消费、是否消费成功、什么时候被消费等等信息。 消息查询：是指能够根据某些信息查询到消息队列中的信息。比如根据消息ID或根据消费位点来查询消息，可以理解为数据库里面的固定条件的select操作。 消息压缩：是指生产端发送消息的时候，是否支持将消息进行压缩，以节省物理资源（比如网卡、硬盘）。压缩可以在SDK完成，也可以在Broker完成，并没有严格限制。通常来看，压缩在客户端完成会比较合理。 多租户：是指同一个集群是否有逻辑隔离，比如一个物理集群能否创建两个名称都为test的主题。此时一般会有一个逻辑概念 Namespace（命名空间）和 Tenant（租户）来做隔离，一般有这两个概念的就是支持多租户。 消息持久化：是指消息发送到Broker后，会不会持久化存储，比如存储到硬盘。有些消息队列为了保证性能，只会把消息存储在内存，此时节点重启后数据就会丢失。 消息流控：是指能否对写入集群的消息进行限制。一般会支持Topic、分区、消费分组、集群等维度的限流。 消息队列的核心模块需要知道的一共是5块：通信协议、网络、存储、生产端、消费端。 pulsar基本理解在 Pulsar 中，一个完整的消费关系通常表达为：**订阅名称-persistent://租户/命名空间/主题**。 订阅名称 (Subscription Name)：- 之前的部分，代表“谁”在消费。 协议与租户&#x2F;命名空间：persistent://xdr-tenant01/asset/。 persistent：数据持久化存储。 xdr-tenant01：租户名（通常对应一个项目或部门）。 asset：命名空间（通常对应一个子系统）。 **主题名称 (Topic Name)**：event_change，代表消息的内容分类。 可以指向同一个订阅： 一份数据，多种用途：底层的资产变更数据只有一份（event_change），但下游有四个不同的功能模块需要它。 进度独立：每个订阅名称都有自己独立的 Cursor（进度指针）。即使“漏洞关联”处理得慢，也不会影响“定时告警”的消费进度。 互不干扰：删除其中一个订阅（比如不再需要 Cron 任务），不会影响其他三个订阅的运行。"},{"title":"【Kubernetes】资源配置相关知识","path":"/2025/12/21/【Kubernetes】资源配置相关知识/","content":"0. 调优思路：何时调内存，何时调 CPU？在实际生产中，调整资源的侧重点通常取决于业务类型： 调大内存： 适用于数据密集型场景。例如：缓存服务（Redis）、大数据处理、内存计算、高并发下的状态存储。内存不足会导致 OOMKilled（进程被强制杀死）。 调大 CPU： 适用于计算密集型或高频请求场景。例如：加解密、压缩、复杂的业务逻辑运算、高频率的 API 调用。CPU 不足会导致 Throttling（限流），表现为接口响应变慢、延迟增加。 1. 什么是 K8s 资源配额？资源配额用于管理 Pod 或 Namespace 对计算资源的使用，确保集群资源合理分配，防止单一服务耗尽集群资源。 资源特性对比 特性 CPU (可压缩资源) 内存 (不可压缩资源) 共享机制 时间片轮转，可多 Pod 并发共享内核 独占，Pod 之间无法共享 超限后果 **限流 (Throttling)**：延时增加，但服务不会中断 **终止 (OOMKilled)**：Pod 直接被强制杀死 单位 核心数 (Cores) 或 毫核 (m) 字节 (Mi&#x2F;Gi) 2. 核心概念：Requests 与 Limits2.1 Requests (请求值) 定义： Pod 运行时所需的最小资源保障。 作用： 调度器 (Scheduler) 根据此值决定将 Pod 调度到哪个节点。 公式： 节点剩余资源 Pod Requests。 2.2 Limits (上限值) 定义： Pod 允许使用的最大资源边界。 作用： 防止因单个 Pod 异常（如内存泄漏）导致整个节点崩溃。 2.3 关系准则 Requests Limits：常规配置。 Requests Limits：资源固定，最稳定。 仅设置 Limits：K8s 会自动将 Requests 设置为与 Limits 相等。 不设置 Limits：Pod 可能无限制消耗节点资源，极具风险。 3. QoS (服务质量) 等级K8s 根据 requests 和 limits 的配置自动划分优先级： QoS 等级 配置条件 稳定性 驱逐优先级 Guaranteed requests &#x3D;&#x3D; limits (所有容器) 最高 最后被驱逐 Burstable 至少一个容器设置了 requests 中等 中间被驱逐 BestEffort 均未设置 requests 和 limits 最低 最先被驱逐 4. 资源配置实践指南4.1 如何确定具体数值？ 监控先行： 通过 Prometheus&#x2F;Grafana 观察应用在压测及生产环境中的 CPU&#x2F;内存峰值与均值。 留有余量： * Requests 正常业务负载的 1.1 - 1.2 倍。 Limits 业务峰值的 1.2 - 1.5 倍。 灰度调整： 根据实际运行情况动态调优，避免资源浪费。 4.2 Namespace 级别管控ResourceQuota (资源配额)限制整个命名空间的总资源。 1234567891011apiVersion: v1kind: ResourceQuotametadata: name: compute-quota namespace: my-namespacespec: hard: requests.cpu: &quot;10&quot; requests.memory: &quot;20Gi&quot; limits.cpu: &quot;20&quot; limits.memory: &quot;40Gi&quot; LimitRange (默认限制)为未配置资源的容器提供默认值，并设定合法范围（Min&#x2F;Max）。 1234567891011121314apiVersion: v1kind: LimitRangemetadata: name: limit-range namespace: my-namespacespec: limits: - default: # 默认 Limit cpu: &quot;1&quot; memory: &quot;512Mi&quot; defaultRequest: # 默认 Request cpu: &quot;500m&quot; memory: &quot;256Mi&quot; type: Container 下一步建议： 如果您想了解如何通过 Vertical Pod Autoscaler (VPA) 自动建议这些资源数值，或者想看看具体的 Prometheus 查询语句来计算这些值，我可以为您提供进一步的指导。"},{"title":"【MongoDB】索引优化","path":"/2025/12/21/【MongoDB】索引优化/","content":"一、卡慢来源1.分析：卡慢感受来源与原因1.1 卡慢感受来源 用户使用前端页面卡慢 Web接口响应慢。 前端渲染逻辑不合理（数据量大时阻塞）。 外部调用 ASM 服务卡慢 对外 API 接口响应慢。 1.2 卡慢原因深度分析（1）MongoDB 问题 内存不足，缓存频繁淘汰，磁盘IO高 大表（如 资产 表）未分片，导致分片间内存使用不均衡。 索引过多且部分不合理，占用大量内存。 IO 压力大 后台任务对 MongoDB 读写频率过高（需增加缓存机制）。 锁竞争（SchemaLock） 库表太多，导致 schemaLock 锁竞争严重，阻塞语句执行。 解决措施：中台调整 WiredTiger Hash 桶；合并小表减少表数量。 （2）后端代码问题 鉴权开销：每个接口都重复请求 auth 认证接口（~100ms）。 级联树依赖：请求级联树接口（缓存过期后耗时 500-600ms）。 接口功能臃肿： 单接口包含过多逻辑（如 on_list 查太多表）。 资产组 接口被多处调用，需求不一但混用同一接口。 业务逻辑复杂&#x2F;不合理： 存在全表扫描统计（如资产清点）。 循环查库代码。 所属主机 逻辑冗余，几乎所有查询都带，导致业务复杂化。 慢查询： 缺乏缓存（实时性不高的统计）。 接口间依赖过重。 （3）前端代码问题 前端逻辑不合理，导致后端返回快但前端渲染慢。 （4）特定环境问题 OpenAPI 频繁请求：请求过于频繁导致 CPU 飙升。 解决措施：开分片；增加限流措施。 1.3 目标及判定标准 网上问题：实现 0 网上卡慢问题。 页面响应速度：保5争3（不超5秒，最好3秒内）。 判定标准：从第一个元素就绪到触发 load 事件的时间（或特定核心 DIV 可视）。 接口响应速度：简单接口 &lt;500ms，复杂接口 &lt;2s。 判定工具：Server-Timing 响应头。 资源占用：降低 Mongo 读写 QPS 及内存占用。 2.优化思路总结这是一个基于你提供的详细报告整理出的优化思路思维导图。 核心策略总结： 做减法：接口字段裁剪、移除冗余业务逻辑（如循环查库）。 做拆分：大接口拆解为多个独立小接口（资产列表）、同步转异步。 按需取：全量加载改为逐级&#x2F;分页加载（资产树、OpenAPI）。 底座优化：Mongo分片、索引治理与锁冲突解决。 12345678910111213141516171819202122232425flowchart TD A[性能问题暴露] A --&gt; B[核心问题类型] B --&gt; B1[请求基础成本高] B --&gt; B2[全量数据返回] B --&gt; B3[接口职责过重] B --&gt; C[统一优化方法] C --&gt; C1[接口拆分] C --&gt; C2[按需加载] C --&gt; C3[分页 / 游标] C --&gt; C4[减少 Join] C --&gt; D[关键接口改造] D --&gt; D1[资产组树&lt;br/&gt;Tree 按需加载] D --&gt; D2[资产列表&lt;br/&gt;主列表瘦身] D --&gt; D3[获取资产组名&lt;br/&gt;全名可选 + 分页] D --&gt; D4[来源设备信息&lt;br/&gt;去循环 + 合并分页] D --&gt; E[最终效果] E --&gt; E1[接口耗时 200-300ms] E --&gt; E2[性能稳定性提升] E --&gt; E3[可扩展性增强] 二、MongoDB 索引治理索引过多不仅会占用大量的内存（WiredTiger 缓存），还会严重拖慢写入性能（Insert&#x2F;Update&#x2F;Delete）。为了解决当前索引冗余、顺序混乱以及查询效率低下的问题，请遵循以下原则： “先规范数据（去 $or），再复用索引（找前缀），最后才考虑新增。” 1. 索引设计核心原则 (ESR)创建复合索引时，字段顺序必须按照 E -&gt; S -&gt; R 的优先级进行排列： **Equal (等值查询)**：最先放置。如 平台id、所属主机 等。 **Sort (排序字段)**：中间放置。如果有 sort 操作，字段必须紧跟在等值字段后。 **Range (范围查询)**：最后放置。如 $gt、$lt、$in、$ne 等。 2. 索引瘦身与复用原则禁止盲目新增索引，优先考虑覆盖与去冗： 利用最左前缀匹配：如果已有索引 &#123;A: 1, B: 1, C: 1&#125;，则无需再为 &#123;A: 1&#125; 或 &#123;A: 1, B: 1&#125; 单独建索引。宽索引可以兼容窄索引的查询。 避免唯一标识冗余：如果 &#123;平台id, 资产_id&#125; 已经能唯一锁定记录，则禁止再为此组合增加 所属主机 等额外字段的复合索引。 全局视角：新增索引前，必须检查现有索引列表。如果通过给现有索引末尾追加字段能解决问题，则不要新建。 3. 数据建模与状态规范（针对 OR 查询问题）禁止在代码中使用复杂的 $or 来兼容多种“初始态&#x2F;空态”： 状态唯一性：一种业务含义只能对应一个值。禁止同时用 null、0、&quot;&quot; 和 None 表示同一种状态。 必须设置默认值：所有新字段在设计时必须明确 Default Value。 ❌ 错误写法：使用 &#123;&quot;$exists&quot;: false&#125; 或 $or 兼容各种空值（这会导致索引扫描效率极低）。 ✅ 正确写法：入库时字段必须存在。查询未处理状态统一使用 &#123;&quot;用户认领状态&quot;: 0&#125;。 严禁逻辑发散：不要在代码里去猜数据可能有哪些空值（如 []、&#123;&#125;），应在写入端保证数据的清洁和规范。 使用 Partial Index 避免无效数据 7. 性能对比：4. 流程规范：先评审，后上线 索引评审制：涉及 MongoDB 索引的新增或修改，必须在文档中列出查询语句并说明理由。 Explain 分析：开发者在申请索引前，需自行执行 .explain(&quot;executionStats&quot;)。 关注点：使用了索引时的状态为stage: &#39;IXSCAN&#39;，totalKeysExamined（扫描索引数）应尽可能接近 nReturned（返回结果数）。 定期清理：DBA 或 Owner 会定期统计索引使用率（db.collection.aggregate([ &#123; $indexStats: &#123;&#125; &#125; ])），对于 Ops（使用次数）为 0 的索引将直接下线。 三、web 接口治理1. 背景与公共问题主要性能瓶颈 Auth 认证：每次请求固定约 100ms 资产组树缓存失效：单次请求 500–600ms 历史接口问题： 大接口一次性返回全部数据 多表 Join &#x2F; 级联查询过多 前端一次性渲染压力大 统一规范 所有接口 必须携带 平台id 禁止使用 &quot;all&quot; 作为默认查询条件（除非明确支持） 2. 资产组树接口重构（Tree 按需加载）改造目标 避免一次性加载全量资产组树 将 “全量返回” → “逐级加载” 核心改动 接口语义升级为 Tree 模式 页面行为调整为： 首次只请求第一层 点击展开节点时，再请求子节点 效果 项目 优化前 优化后 接口耗时 8–9s ~300ms 加载方式 全量 按需 用户体验 卡顿 秒开 3. 资产列表接口重构（拆接口）改造目标 列表接口只做“核心资产数据查询” 所有关联信息拆成独立接口 核心策略 List 主接口 只查 资产 表 + 必要配置表 不再 Join 资产组 &#x2F; 组织 &#x2F; 平台 &#x2F; 业务等信息 关联信息 按需调用 前端或中台统一聚合 拆分出的独立能力 资产组名称 &#x2F; 全名 MAC 来源 组织架构全名 资产认领信息 业务组名称 平台名称 性能收益 环境 优化前 优化后 ~6s ~300ms 157万条数据 ~600ms ~200ms 4. 性能收益 维度 旧接口 (已弃用) 新接口 (推荐&#x2F;合一分支) 性能提升 资产列表 URL ?method=GET ?method=GET 23倍 资产组树 URL ?method=get ?method=get&amp;_style=tree 20倍+ 资产组 响应耗时 7.9s ~ 9s 300ms ~ 420ms 显著减少延迟 On_list 响应耗时 2.38s 103ms 亚秒级实时返回 核心逻辑耗时 - 约 100ms (其余为Auth认证) 逻辑大幅精简 性能基线要求 无明确标准 资产组 &lt; 500ms &#x2F; On_list &lt; 200ms 建立硬性准入基线 接口平均耗时下降 5～30 倍 核心列表接口稳定在 200–300ms 架构从「胖接口」演进为「能力解耦型接口」 为后续缓存 &#x2F; 并发 &#x2F; 异步聚合提供空间"},{"title":"【通用工具】Makefile：自动化编译说明书","path":"/2025/12/20/【通用工具】Makefile：自动化编译说明书/","content":"这是什么？Makefile 是一个自动化编译工具的“说明书”。 在开发大型软件项目时，通常会有成百上千个源代码文件。如果你每次改动一点代码都要手动输入命令去编译每一个文件，不仅低效，还容易出错。Makefile 的出现就是为了解决这个问题。 1. Makefile 的核心作用 自动化编译：只需输入一个简单的命令（通常是 make），它就会根据 Makefile 里的规则，自动调用编译器（如 gcc, g++）完成整个项目的构建。 增量编译（按需编译）：这是它最强大的地方。Makefile 会检查文件的时间戳。它只编译那些被修改过的文件以及受其影响的文件，而不会浪费时间去重新编译没变动的部分。 管理依赖关系：它能清晰地定义文件之间的依赖关系。比如 A 文件引用了 B 文件的函数，当 B 改变时，Makefile 知道必须重新编译 A。 2. Makefile 的基本结构Makefile 的核心逻辑由一系列“规则”组成，格式如下： 12目标(target): 依赖文件(prerequisites)\t命令(command) # 注意：前面必须是一个 Tab 制表符 举个栗子：假设你有一个项目，包含 main.c 和 tool.c 两个文件。 123456789101112131415# 最终目标是生成可执行文件 my_programmy_program: main.o tool.o\tgcc -o my_program main.o tool.o# main.o 依赖于 main.cmain.o: main.c\tgcc -c main.c# tool.o 依赖于 tool.ctool.o: tool.c\tgcc -c tool.c# 清理编译产生的垃圾文件clean:\trm *.o my_program 3. 为什么现在还要学它？虽然现在有很多现代化的构建工具（如 CMake, Bazel），但 Makefile 依然是基石： 工程必备：绝大多数 Linux 内核和开源项目（如 Redis, Nginx）都直接使用 Makefile。 跨语言通用：虽然它常用于 C&#x2F;C++，但其实你可以用它管理任何文件的转换任务（比如自动压缩图片、编译 LaTeX 文档等）。 CMake 的底层：在 Linux 环境下，CMake 通常也是先生成 Makefile，然后再调用 make 来执行。 4. 总结：它的价值 维度 手动编译 使用 Makefile 速度 每次都要全量编译，慢 仅编译变动部分，快 准确性 容易漏掉某个文件 严格按依赖逻辑，准 复杂度 命令又长又难记 一个 make 搞定 为啥python项目里也有？按理说，Python 是解释型语言，不需要像 C&#x2F;C++ 那样经历复杂的“编译”过程，但 Makefile 在 Python 项目中依然非常流行，原因在于：它把 Makefile 当作了一个“快捷指令集（Task Runner）”来使用。 在 Python 开发中，Makefile 主要解决的是“命令太长记不住”和“环境初始化太繁琐”的问题。 1. Python 项目中 Makefile 的常见用途Python 项目虽然不编译，但有很多重复性的“脏活累活”： 环境管理：创建虚拟环境、安装依赖包。 清理垃圾：删除生成的 __pycache__、.pyc 文件或构建产物。 质量检查：运行代码格式化（black）、静态检查（flake8）、类型检查（mypy）。 运行测试：启动 pytest 并生成覆盖率报告。 打包发布：构建 Docker 镜像或上传包到 PyPI。 2. 一个典型的 Python Makefile 示例你可以看看你项目里的 Makefile，是不是很像下面这种结构： 1234567891011121314151617181920212223242526# 定义变量PYTHON = python3PIP = pip# 1. 安装依赖install:\t$(PIP) install -r requirements.txt# 2. 运行测试test:\tpytest tests/# 3. 代码格式化format:\tblack .\tisort .# 4. 清理 Python 产生的临时文件clean:\tfind . -type d -name &quot;__pycache__&quot; -exec rm -rf &#123;&#125; +\trm -rf .pytest_cache\trm -rf dist# 5. 一键启动项目run:\t$(PYTHON) main.py 这样一来，你只需要输入 make test 就能跑测试，输入 make clean 就能清理环境，而不需要记住后面那一串复杂的参数。 3. 为什么不用 Shell 脚本（.sh）？既然只是快捷指令，为啥不用 .sh 脚本呢？ 统一入口：Makefile 提供了一个标准的入口。不管新同事是用什么系统，只要看到 Makefile，就知道输入 make install 就能开始干活。 依赖逻辑：Makefile 依然可以处理依赖。例如，你可以规定：运行测试前必须先安装依赖。 12test: install pytest 当你执行 make test 时，如果 install 没跑过，它会自动先跑 install。 习惯使然：在 Linux&#x2F;Unix 世界里，make 是装机自带的，不需要额外安装工具。 4. 现代替代品虽然 Makefile 很香，但现在 Python 社区也有一些更“原生”的替代方案： Invoke: 用纯 Python 写的任务执行器。 Poetry&#x2F;Pdm: 自带了部分脚本管理功能。 Tox&#x2F;Nox: 专门用于管理多版本 Python 测试环境。 项目实战整体层级结构123456789101112131415161718192021222324252627282930313233343536373839graph TD %% 第一层：Makefile 入口 subgraph Makefile_Layer [Makefile 指挥层] A[make build] --&gt;|调用| B1(build.sh) A1[make enc_build] --&gt;|带参数 -u| B1 A2[make cov_build] --&gt;|带参数 -c| B1 A3[make pythonlib] --&gt;|调用| B2(pythonlib.sh) end %% 第二层：脚本逻辑层subgraph Script_Layer [Shell 脚本执行层] B1 --&gt; C1&#123;解析参数&#125; C1 --&gt;|默认| D1[标准编译: go build] C1 --&gt;|-u| D2[加密编译: garble build] C1 --&gt;|-c| D3[覆盖率编译: goc build] B2 --&gt; E1[清理旧包: clean] E1 --&gt; E2[生成 RPC 代码: protoc] E2 --&gt; E3[修复导入路径: sed] E3 --&gt; E4[打包: build.sh]end%% 第三层：细节处理subgraph Build_Detail [编译细节注入] D1 &amp; D2 &amp; D3 --&gt; F1[注入版本信息: -ldflags] F1 --&gt; F2[遍历 cmd/* 目录] F2 --&gt; F3[检查 main.go]end%% 第四层：最终产物subgraph Output_Layer [最终产物] F3 --&gt; G1[dist/ 目录二进制文件] E4 --&gt; G2[Python Wheel/Egg 包] G1 --&gt; G3[写入 CI_COMMIT_SHA 等元数据]end%% 样式美化style Makefile_Layer fill:#f9f,stroke:#333,stroke-width:2pxstyle Script_Layer fill:#bbf,stroke:#333,stroke-width:2pxstyle Output_Layer fill:#bfb,stroke:#333,stroke-width:2px 要点解析： 参数透传：Makefile 里的不同命令（如 enc_build）本质上是在调用 build.sh 时塞入了不同的“旗标”（Flag），比如 -u。 环境准备：在 build.sh 内部，它会先通过 source 加载环境，确保 Go 版本正确，并自动下载缺少的工具（如 garble 或 grpcurl）。 版本溯源：无论走哪条路径，最终都会经过 VER_FLAGS 注入。这意味着你拿到的任何一个二进制文件，都能通过内部变量追溯到它是哪次 Git 提交。 并行与依赖：Makefile 保证了“先清理、后编译”的逻辑顺序（例如 pythonlib 依赖 clean），而 build.sh 则负责具体的“体力活”（循环遍历目录并一个个编译）。 有了这张图，当你下次需要定位编译报错时，只需看报错阶段处于哪个层级，就能快速锁定是 Makefile 规则写错了，还是 Shell 脚本逻辑有问题。 生成其他项目的依赖包注意，两个build.sh并不一样，idt的那个只是用来生成wheel文件的！生成完之后可以上传到内部的镜像仓库。 重点是这个语句：python3 setup.py bdist_wheel，其中setup.py自己写，一般是去指定版本号等，避免重复。 上传命令：[凭证环境变量] rsync [参数] [源文件] [协议]://[用户名]@[地址]/[目标模块]/ 例如，RSYNC_PASSWORD=&lt;密码&gt; rsync a.txt rsync://[用户名]@[地址]/[目标模块]/ 整个完整周期为： 1234567891011121314151617181920212223242526272829303132333435graph TD %% 第一阶段：代码生成 subgraph Stage1 [1. 接口转换阶段] A[identifier.proto] --&gt;|grpc_tools.protoc| B[PB 代码生成] B --&gt; B1[identifier_pb2.py] B --&gt; B2[identifier_pb2_grpc.py] B1 &amp; B2 --&gt; C[sed 修正导入路径] end %% 第二阶段：元数据准备 subgraph Stage2 [2. 元数据准备阶段] D[&quot;idt/__init__.py&quot;] --&gt;|提供名称/作者| E[setup.py] F[datetime.now] --&gt;|提供日期后缀| E C --&gt;|放入包目录| G[&quot;idt/pb/&quot;] end %% 第三阶段：构建打包 subgraph Stage3 [3. 构建打包阶段] E --&gt;|python3 setup.py bdist_wheel| H&#123;Setuptools 工厂&#125; G --&gt; H H --&gt; I[build/ 临时编译目录] H --&gt; J[idt.egg-info/ 依赖记录] H --&gt; K[dist/xxxx.whl 最终成品] end %% 第四阶段：分发 subgraph Stage4 [4. 远程分发阶段] K --&gt; L[rsync 上传] L --&gt; M[&quot;www.ngsoc.site/pip/&quot;] M --&gt; N[其他同事: pip install] end %% 样式美化 style Stage3 fill:#f96,stroke:#333 style K fill:#00ff00,stroke:#333,stroke-width:4px 而使用侧只需要在其脚本中指定版本号，就能下载到最新的wheel包。一般就是通过前面的版本号，&gt;&#x3D;某个版本号，就会自动拉取到最新的时间戳对应的wheel包。 1234567891011121314151617181920graph TD subgraph Your_Side [你的侧: 产出] A[make pythonlib] --&gt; B[生成 whl: v1.0.0.20231215] B --&gt; C[rsync 上传至镜像站] end subgraph Mirror_Site [镜像站/文件服务器] C --&gt; D[www.ngsoc.site/pip/ 下的文件列表] end subgraph Colleague_Side [同事侧: 消费] E[运行管理脚本] --&gt; F&#123;版本号逻辑判断&#125; F --&gt;|符合区间 &gt;= v1.0| G[从镜像站拉取对应 whl] G --&gt; H[静默安装: pip install idt.whl] H --&gt; I[启动业务程序] end style D fill:#f9f,stroke:#333 style F fill:#bbf,stroke:#333 style H fill:#dfd,stroke:#333 整个项目的build.sh的流程​ 1234567891011121314151617181920212223242526272829303132333435363738flowchart TD Start([开始执行 scripts/build.sh]) --&gt; Init[初始化路径 CURDIR/PRODIR] Init --&gt; LoadCommon[Source 加载 common/devenvrc 配置]LoadCommon --&gt; ParseArgs&#123;getopts 解析参数&#125;ParseArgs --&gt;|无参数| Default[默认: 标准编译]ParseArgs --&gt;|-o| SetOut[指定输出目录]ParseArgs --&gt;|-c| SetCov[开启覆盖率模式 cov=1]ParseArgs --&gt;|-u| SetEnc[开启加密模式 encFlag=1]SetOut &amp; SetCov &amp; SetEnc &amp; Default --&gt; PreStep[执行 GO_MOD_TIDY &amp; install_tools]PreStep --&gt; CovCheck&#123;是否开启覆盖率?&#125;CovCheck --&gt;|Yes| DownloadGoc[下载 goc 工具]CovCheck --&gt;|No| ScanDir[遍历 cmd/ 和 tools/ 目录]DownloadGoc --&gt; ScanDirScanDir --&gt; LoopStart[循环每一个 app 子目录]LoopStart --&gt; CheckMain&#123;是否存在 main.go?&#125;CheckMain --&gt;|No| Skip[LOG WARN: 跳过该目录]CheckMain --&gt;|Yes| Version[构造 VER_FLAGS: 注入版本/Git/日期]Version --&gt; BranchMode&#123;判断编译模式&#125;BranchMode --&gt;|cov=1| BuildCov[调用 goc build 编译]BranchMode --&gt;|encFlag=1| BuildEnc[安装并运行 garble 混淆编译]BranchMode --&gt;|Default| BuildStd[标准 go build -ldflags]BuildCov &amp; BuildEnc &amp; BuildStd --&gt; Install[install 命令创建目录并移动二进制文件]Install --&gt; LoopNext&#123;是否有下一个 app?&#125;LoopNext --&gt;|Yes| LoopStartLoopNext --&gt;|No| End([输出 LOG INFO: Build Success!])%% 样式美化style BranchMode fill:#f96,stroke:#333style ParseArgs fill:#bbf,stroke:#333style BuildEnc fill:#f66,stroke:#333 这个 build.sh 脚本是整个 Go 项目的核心编译大脑。它的任务不仅仅是运行 go build，还包含了版本注入、代码混淆、覆盖率统计和工具链管理等工业级生产任务。 以下是该脚本的详细功能拆解： 1. 核心编译模式（三种模式）脚本通过 getopts 接收参数（如 -c, -u），决定了程序以哪种方式“出生”： 模式 触发参数 使用的工具 核心目的 标准模式 默认 go build 正常的开发与生产环境运行。 代码混淆模式 -u (encrypt) Garble 防止逆向工程。通过混淆代码中的字面量和函数名，让别人拿到二进制文件也难以反编译。 覆盖率模式 -c (coverage) Goc 自动化测试。配合测试平台统计代码运行到了哪几行，通常用于 QA 或集成测试阶段。 2. 关键功能点解析A. 动态版本注入 (VER_FLAGS)脚本中有一段很长的 VER_FLAGS，利用了 Go 的 -ldflags 特性： 1-X sangfor.com/xdr/go-idt/internal/version.version=$&#123;PROJECT_VERSION&#125; ... 作用：在编译时，强行把当前代码的版本号、Git 提交记录（Commit ID）、编译日期等信息塞进 Go 的变量里。 效果：当你运行 ./my_app -v 时，它能准确报出自己是哪天在哪个分支编译的。 B. 自动化扫描与构建脚本并没有写死要编译哪个文件，而是遍历了 cmd/ 和 tools/ 目录： 12apps=$(ls -d -- $&#123;inDir&#125;/*/)for app in $&#123;apps&#125;; do ... 逻辑：它会自动寻找这两个文件夹下的每一个子目录。只要目录下有 main.go，它就会为其创建一个文件夹并完成编译。 好处：你每增加一个微服务或工具，只需在 cmd/ 下建个文件夹，脚本会自动识别并打包。 C. 工具链自给自足 (install_tools)脚本会检查并安装 grpcurl（一个类似 curl 但用于调试 gRPC 的工具）： 逻辑：如果 dist 目录下没有这个工具，它会自动 go install 并在本地备份。这保证了任何开发环境只要运行一次编译，调试工具就全齐了。 3. 脚本执行流程图 环境检查：加载 devenvrc，指定使用 Go 1.21 版本。 清理与预备：运行 go mod tidy 整理依赖。 判断路径： 如果带了 -u：安装并执行 garble 混淆编译。 如果带了 -c：从内部仓库（Nexus）下载 goc 并进行带统计信息的编译。 默认：执行 go build -ldflags &quot;-s -w&quot;（其中 -s -w 是为了减小生成的二进制体积，删掉调试符号）。 产物输出：所有编译好的二进制文件会被统一放在 dist/ 目录下。 4. 总结：为什么要写这么复杂的脚本？如果你只写一个 go build，你得到的是一个“裸”的程序。而通过这个脚本，你得到的是： 可追踪的（知道是谁在什么时候编译的）。 安全的（如果是发布给外部，代码已被混淆）。 可监控的（支持覆盖率测试）。 标准化的（所有开发者产出的二进制格式完全一致）。 你想知道在 Makefile 里是如何给这个脚本传递 -u 或 -c 参数的吗？（提示：这对应你之前看到的 enc_build 和 cov_build 目标）"},{"title":"【go基础】协程管理","path":"/2025/12/06/【go基础】协程管理/","content":"从一个错误案例开始起因是在工作中看到一段代码，感觉是程序遇到异常而卡住的原因，起了两个协程，其中一个等待另一个完成后开始，二者通过channel进行消息传递。后一个协程在完成时使用了goto语句。程序最后为select{}永久卡死，等待前面启动的两个协程去跑完就完事了。 原来的代码（铁有问题，不能用在生产上）： 123456789101112131415161718\tticker := time.NewTicker(5 * 60 * time.Second)\tdefer ticker.Stop()loop:\tfor &#123; select &#123; case &lt;-h.ctx.Done(): mlog.Info(&quot;asset infer handler stop&quot;) return case &lt;-ticker.C: if h.NeedRun() &#123; go h.AssetInfer() go h.GenMiddleTable() break loop &#125; &#125;\t&#125;\tselect &#123;&#125;&#125; 用ai帮忙修改了相应的程序，但是其写的代码在go包装协程中启动了工作协程： 123456789101112131415161718192021222324\tticker := time.NewTicker(5 * 60 * time.Second)\tdefer ticker.Stop()\tvar wg sync.WaitGroup\tfor &#123; select &#123; case &lt;-h.ctx.Done(): mlog.Info(&quot;asset infer handler stop&quot;) return case &lt;-ticker.C: if h.NeedRun() &#123; wg.Add(2) go func()&#123; defer wg.Done() go h.AssetInfer() &#125;() go func()&#123; defer wg.Done() go h.GenMiddleTable() &#125;() wg.Wait() &#125; &#125;\t&#125;&#125; 这个程序还是会有问题，用 WaitGroup 启了两个 wrapper 协程，wrapper 里又 go 出实际工作协程，导致 WaitGroup 很快结束，真正的工作协程无人管理，后续 tick 还会继续再起新协程，既难以追踪生命周期也可能造成 goroutine 泄漏。 1.不允许在go 协程里面嵌套协程 2.必须要遵守一处并发、统一等待的原则 修改后的程序： 1234567891011121314151617181920212223ticker := time.NewTicker(5 * 60 * time.Second)defer ticker.Stop()var wg sync.WaitGroupfor &#123;\tselect &#123;\tcase &lt;-h.ctx.Done(): mlog.Info(&quot;asset infer handler stop&quot;) return\tcase &lt;-ticker.C: if h.NeedRun() &#123; wg.Add(2) go func() &#123; defer wg.Done() h.AssetInfer() &#125;() go func() &#123; defer wg.Done() h.GenMiddleTable() &#125;() wg.Wait() &#125;\t&#125;&#125; 进一步，还可以使用 errgroup 进行并发任务管理：waitGroup–&gt;errGroup 原来的：sync package - sync - Go Packages 简化后：errgroup package - golang.org&#x2F;x&#x2F;sync&#x2F;errgroup - Go Packages 1234567891011121314151617181920212223242526 ticker := time.NewTicker(5 * 60 * time.Second) defer ticker.Stop()\tfor &#123; if h.NeedRun() &#123; g, _ := errgroup.WithContext(h.ctx) g.Go(func() error &#123; h.AssetInfer() return nil &#125;) g.Go(func() error &#123; h.GenMiddleTable() return nil &#125;) if err := g.Wait(); err != nil &#123; mlog.Warnf(&quot;asset infer run exit: %v&quot;, err) &#125; return &#125; select &#123; case &lt;-h.ctx.Done(): mlog.Info(&quot;asset infer handler stop&quot;) return case &lt;-ticker.C: &#125;\t&#125;&#125; 不需要，原来的loop版本，就是等待NeedRun，为true，就去执行两个协程，然后 [mysql] 2025&#x2F;11&#x2F;22 23:04:45 packets.go:36: unexpected EOF{“level”:”warn”,”time”:”2025-11-22T23:04:45.769+0800”,”position”:”handler&#x2F;asset_infer_handler.go:848”,”function”:”sangfor.com&#x2F;xdr&#x2F;asm-cron&#x2F;internal&#x2F;assetinfer&#x2F;handler.(AssetInferHandler).genMiddleResult”,”message”:”query:SELECT dstAssetId, regexp_extract(responseBody, ‘(.?)‘, 1) as title, CAST(count(*) AS BIGINT), max(recordTimestamp) as maxRecordTimestamp, any_value(if(instr(url, ‘?’) &gt; 0, substring(url, 1, instr(url, ‘?’) - 1), url)) As url FROM business.http_log WHERE respStatus &#x3D; 200 AND recordTimestamp &gt;&#x3D; 1763654400 AND recordTimestamp &lt; 1763740800 AND respContentType &#x3D; ‘text&#x2F;html’ AND instr(responseBody, ‘‘) &gt; 0 GROUP BY dstAssetId, title LIMIT 300000;, query failed,err:Error 1064: Failed to allocate resource to query: org.apache.thrift.transport.TTransportException: Socket is closed by peer., will retry 3 times”}"},{"title":"【数据库】存算一体到存算分离","path":"/2025/12/03/【数据库】存算一体到存算分离/","content":"1.结论哪个好？简短的回答是：没有绝对的“好”，只有“更适合”。 存算一体（Coupled）：胜在 极致性能（尤其是低延迟）。 存算分离（Disaggregated）：胜在 弹性、成本和灵活性。 目前的行业大趋势是：从“存算一体”向“存算分离”演进，特别是在云原生（Cloud Native）和大数据分析（OLAP）领域。但在对延迟极度敏感的交易型（OLTP）场景中，存算一体依然是主流。 核心区别 特性 存算一体 (Shared-Nothing &#x2F; Coupled) 存算分离 (Shared-Storage &#x2F; Disaggregated) 架构逻辑 每个节点既负责计算（CPU&#x2F;内存），又负责存储（本地磁盘）。 计算节点只负责计算（无状态），数据存储在独立的分布式存储池（如 S3, HDFS, EBS）。 扩容方式 耦合扩容：想加磁盘就必须买新机器（哪怕CPU是空闲的）；想加CPU就得买新机器（哪怕磁盘是空的）。 独立扩容：计算不够加计算节点，存储不够加存储容量。互不干扰。 数据迁移 困难：节点下线或扩容时，需要大量的数据Rebalance（搬迁），由于涉及物理拷贝，非常慢且影响性能。 极快：数据在远端存储，计算节点无状态。新增节点只需挂载数据即可，几乎秒级完成。 性能 (延迟) 极高：利用“数据本地性”（Data Locality），CPU直接读本地盘，网络开销小。 中等：所有数据读写都要走网络（Network I&#x2F;O），受限于网络带宽和延迟。 成本 较高：资源利用率低（存在短板效应），且通常需要高性能本地盘。 较低：可以使用廉价的对象存储（如 AWS S3, OSS），且计算资源可以按需启停（Serverless 基础）。 演进的核心原因 现在的网络非常快（100Gbps&#x2F;RDMA），网络不再是绝对瓶颈。 企业上云后发现“存算一体”太僵硬了。 主流数据库1. 交易型数据库 (OLTP)主流现状：主要还是存算一体，但云厂商正在推存算分离。 传统派（存算一体）： **MySQL, PostgreSQL, Oracle (单机&#x2F;非RAC)**。 原因： 交易系统（如下单、支付）对延迟要求极高（毫秒级），本地磁盘读写依然是最稳的。 云原生派（存算分离）： Amazon Aurora, 阿里云 PolarDB, Google AlloyDB。 架构： 它们也是 MySQL&#x2F;PG 协议，但底层把存储剥离到了共享存储层（Shared Storage）。 优势： 一写多读，故障恢复极快（不用拷贝数据），存储自动扩容。 2. 分析型数据库 (OLAP) &#x2F; 数据湖主流现状：已全面转向存算分离。 绝对标杆： Snowflake。它是存算分离架构的鼻祖和推广者，计算节点完全无状态，数据全在 S3 上。 大数据体系： Databricks (Spark), Presto&#x2F;Trino。天然就是存算分离的，计算引擎查 S3&#x2F;HDFS 数据。 国产&#x2F;开源新秀的转型： StarRocks &#x2F; Doris： 最初是存算一体（为了极致性能），但现在最新的版本都大力主推“存算分离模式”（Shared-data mode），为了降低成本和提高弹性。 ClickHouse： 典型的一体架构（本地表），性能极快但运维极其痛苦（扩容难）。目前也在尝试通过 S3 存储策略实现分离，但架构包袱较重。 3. 分布式数据库 (NewSQL) TiDB &#x2F; OceanBase： 逻辑上是分离的（KV存储层 TiKV 和 SQL计算层 TiDB 是分开的），但部署上通常为了性能建议混合部署或走专用网络。 如何选择？选“存算一体”： 如果你的业务是核心交易系统，或者数据量相对固定（TB级），且对查询延迟（Latency）要求在毫秒级。 代表： MySQL, ClickHouse (Local模式), Redis。 选“存算分离”： 如果你的业务是海量数据分析（PB级），数据量增长极快，负载波动大（白天忙晚上闲），或者希望降本增效。 代表： Snowflake, StarRocks (存算分离模式), 云原生数据库 (Aurora&#x2F;PolarDB)。 2.ClickHouse本地表与分布式表 表类型 本地表 (Local Table) 分布式表 (Distributed Table) 是否存真实数据 是 (YES) 否 (NO) 本质 仓库。物理文件真实地存在磁盘上。 传送门&#x2F;路由器。它只是一个视图（View），本身不存数据，只存“配置信息”。 作用 负责存数据、压缩数据、通过索引找文件。 负责把请求“分发”给各个节点，并把结果“汇总”起来。 对应关系 N 个节点就有 N 个本地表。 集群里逻辑上只有一个入口。 查询 (Read)： 必须查分布式表！ 如果你查本地表，你只能查到当前连接的那个节点的数据（比如 1&#x2F;3 的数据），结果是不对的。 只有查分布式表，CK 才会帮你去所有节点把数据捞一遍并汇总。 写入 (Write)： 这里有分歧（重点） 方案 A（官方建议&#x2F;简单）：写分布式表。 代码直接 insert 到 test_table_all。 优点： 简单，不用管路由。 缺点： 数据会先到一个节点，然后再次转发到其他节点，写入性能有损耗，且如果那个转发节点挂了，可能会丢数据或卡住。 方案 B（高性能&#x2F;老鸟做法）：写本地表。 代码里通过负载均衡（Load Balancer）或者自己在代码里算 Hash，把数据直接 insert 到各个节点的 test_table_local。 优点： 性能最强，没有二次转发。 缺点： 代码复杂，你需要知道后端有多少个 IP 地址。"},{"title":"【分布式】节点数为什么设置成奇数？","path":"/2025/11/29/【分布式】节点数为什么设置成奇数？/","content":"多数派的定义分布式系统里（比如 Etcd、ZooKeeper、Consul、Redis Sentinel、MongoDB ReplicaSet 等）一般采用多数派原则。 多数派原则（majority）： 只有多数派节点（majority of voting nodes）同意，写入才算成功；只有多数派在线，集群才能选出主节点。少数派分区永远不能参与选举，也不能投票 选举（leader election）和一致性仲裁（quorum）更高效、更可靠。在最少节点的情况下达成最高的可用性。 多数派的定义是： 1quorum = floor(N/2) + 1 多数派选举需要 &gt;50% 的票数 奇数节点能减少浪费的资源 偶数节点不会提高容错能力 奇数节点能避免“平票”（leader 选不出来） 奇数投票节点确保一致性？（1）避免平票，保证能选出 Primary奇数节点让投票在任何情况下都能明确区分“多数派”和“少数派”，而不会出现 50&#x2F;50 的对等分裂，从而避免平票，让 leader 一定能选出来。 主节点选举依赖多数票。示例对比： ✔ 使用 3 个投票节点（奇数）！ 123456节点：A, B, C （3 voting）majority = 2假设 A 挂了：剩下 B、C 仍有 2 票可继续选主 系统仍然 有主节点 → 可继续读写。 ✘ 使用 4 个投票节点（偶数） 12节点：A, B, C, Dmajority = 3 即使挂掉一个： 1剩 3 个节点 → 需要 3 票 → 刚好全部在线 任何一个节点再出现网络抖动： 12只剩 2 票 &lt; majority(3)无法选主 → 集群只读不能写 ➡ 多花了一个节点却不能提升容错能力。 （2）写入确保不会产生“脑裂（split-brain）”写操作要求被多数投票节点确认才能成功： 举例：3 voting nodes → majority &#x3D; 2 1primary 成功写入两个节点 → 数据安全 这样即使 primary 发生故障，另一个节点晋升为 primary 时： 它一定已经拥有最新多数写入的数据 旧主不会带着未提交的数据乱写 ➡ 奇数投票节点保证：只有一个分区能达到 majority，另一个分区不能产生脏写。 （3）最优容错&#x2F;资源比 建议投票节点必须为奇数（3、5、7） 是因为偶数节点没有增加容错能力，纯浪费资源。 节点数 多数派 容忍故障数 说明 3 2 1 常用最小集群 4 3 1 和 3 节点一样，但多浪费一个节点 5 3 2 容错能力提高了 ➡ 所以为了不浪费资源，同样想增加容错能力时，会直接跳到 5 节点，而不是 4 节点。 primary 必须获得多数投票 → 避免选举平票 写操作必须多数节点确认 → 避免多主和脑裂 奇数节点最大化可用性，最小化资源浪费 因此 MongoDB 通过 majority 写入 + 奇数投票节点 + 单主架构 确保副本集中只有一个合法主节点，并且数据是强一致的。 主节点失联 注意：网络分裂是“触发选举”的原因 副本集的节点之间依赖心跳（heartbeat）互相确认是否“能连通”。如果 Primary 无法与多数投票节点通信，它必须自动放弃主节点身份。自动降级（step down）变成 Secondary、不参与选举、不参与投票。 否则：它写的数据无法同步给其他节点，其他节点可能已经选出新的 Primary，两个 “主” 会产生冲突写，破坏一致性。 能连通 &#x3D; 存活节点 不能连通 &#x3D; 故障节点 故障节点 自动被投票系统忽略。只有与大多数节点相互可达的节点才是合法的投票者。选举只发生在多数派分区中。 当主节点恢复，发现已经有新的 Primary，不会争抢主，会自动作为 Secondary 追赶 oplog（同步数据），完全不会参与正在进行的选举。 选举新主节点 Phase 1：请求成为候选人（申请 term） Phase 2：投票 在任何一个 term（选举轮次）里只能有一个候选人。 🚫 他们无法同时成为候选人（协议禁止） 🚫 electionTimeout 随机化让“同时发起”几乎不可能，即使一轮失败，也不会卡住，而是发起下一轮选举投票 🚫 即使同时尝试，term 冲突规则保证只有一个候选人存活 🚫 投票只能投给一个候选人，不能“投给自己”当候选人 Raft 用的是 随机选举超时，每个节点有一个随机 electionTimeout，这个时间到达之前，不会发起选举，所以每轮只有 1 个节点最先到时间。 即使时间因为延迟等原因差不多，Raft 规定节点只能给一个候选人投票。 B 向 C 请求投票 C 向 B 请求投票 两者收到对方的 term 后，会比较 term 大小：谁的 term 更大，另一个就自动放弃本次竞选（step down），票投给 term 更大的那一方。所以最终仍然会选出一个 Primary。这叫 term 冲突决议。 什么情况下会卡住？ 多数派本身断裂：A /断网/ B C，集群仍然正常运行（在 B、C 那一边） 如果分裂成：A B | C，AB都在选自己，C又断网→无法投票，本轮失败 → 下一轮继续随机超时 → 必然选出一个。 有多数派 → 一定能选出来 没多数派 → 协议自动重试，直到选出（除非网络把多数派都断开，否则不会选不出来。） 节点故障和网络分区本质一样，但表现方式不同。 节点故障 &#x3D; 无法与该节点通信 网络分区 &#x3D; 一部分节点无法与另一部分通信 → 从 Raft &#x2F; Paxos &#x2F; Zookeeper 的选主协议视角来看：两者都是“通信不可达”，本质等价。 1. 节点故障属于网络分区的一种特殊情况节点故障有两种典型表现： 节点宕机（进程挂了、机器断电） 节点卡死（CPU 100%，没有响应） 在这两种情况下： 其他节点发 RPC 给它 超时，一直收不到回复 效果 ≈ 这个节点被隔离到了一个别人都到不了的区域 这就是一个非常典型的 “一对多” 网络分区： 12正常区域：A、B、C隔离区域：D（单独一块） 从多数派算法看： D 和大家无法通信 A、B、C 仍然构成多数派，继续运行 所以“节点故障”其实就是“单节点网络分区”。 2. 网络分区不一定是节点故障网络分区可以更复杂，比如： 情况 1：分成 2-2 的两个组（4 节点例子） 1A B | C D 两边互相看不到 每一边都认为另一边“宕机” 但实际上大家都还在，只是通信断了。 情况 2：单链路损坏 123A -- CA X BB -- C A 和 B 互相到不了，但都能到 C。 这不是任何节点故障，只是链路坏了。 3. 多数派协议看的是“能否通信”，不看“为什么无法通信”对于 Raft &#x2F; Paxos &#x2F; ZK 而言： 情况 协议视角 等价吗 节点宕机 收不到心跳 → 不可达 和“单节点分区”完全一样 节点高负载卡死 超时 → 不可达 等价 网络 cable 掉了 不可达 等价 路由异常导致某节点看不到其他节点 不可达 等价 整个机房掉电 大部分节点不可达 等价 所以：对分布式一致性算法来说，“原因不重要”，只看通信结果。 4. 为什么要把它们视为同一种情况？因为共识算法的核心目标是： 保证只有多数派区域能够选主、写数据。 不管是因为： 节点宕机 火灾 网卡坏了 交换机掉线 延迟过高 TCP 拥塞太严重 机器卡死不回包 某段链路丢包 最终的结果都是节点之间无法通信。所以从一致性协议角度： 节点故障”和“网络分区”都是“不可达”。 它们的后果与处理方式完全一样。 拓展问题MongoDB 如何判断一个节点“失联”？（基于心跳、electionTimeout） 投票节点（voting node）和数据节点（non-voting node）的区别？ 如果主节点没有 step down 会怎样？（真实脑裂案例） “网络分裂情况下的时间线”描述一次真实的 MongoDB 选举过程"},{"title":"【mongoDB】分片","path":"/2025/11/28/【mongoDB】分片/","content":"1.实战中的配置查看mongo集群的pod12345678910kubectl get pod -n mongoNAME READY STATUS RESTARTS AGEmongo-mongodb-sharded-configsvr-0 1/1 Running 0 16dmongo-mongodb-sharded-configsvr-1 1/1 Running 1 (16d ago) 16dmongo-mongodb-sharded-configsvr-2 1/1 Running 1 (16d ago) 16dmongo-mongodb-sharded-mongos-0 1/1 Running 0 16dmongo-mongodb-sharded-mongos-1 1/1 Running 0 16dmongo-mongodb-sharded-shard0-data-0 1/1 Running 0 16dmongo-mongodb-sharded-shard0-data-1 1/1 Running 0 16dmongodb-exporter-976c9b7b-8dt25 1/1 Running 0 16d 在 MongoDB 分片架构中： mongos (mongo-mongodb-sharded-mongos-0): 只是路由节点，负责转发请求，不存储任何数据。所以你在它的 describe 结果里只看到了 localtime 和 kube-api-access，看不到数据卷。 configsvr (mongo-mongodb-sharded-configsvr-0): 存储集群的元数据（配置信息）。 shard data (mongo-mongodb-sharded-shard0-data-0): 这才是真正存储数据库业务数据的地方。 想要知道数据实际存储在哪里第一步：查看数据节点 Pod 的 PVC 名称 1kubectl describe pod mongo-mongodb-sharded-shard0-data-0 -n mongo 找到Volumes部分，其中ClaimName就是PVC名字。 12345Volumes: datadir: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: datadir-mongo-mongodb-sharded-shard0-data-0 ReadOnly: false 第二步：查看 PVC 对应的 PV 名称 1kubectl get pvc datadir-mongo-mongodb-sharded-shard0-data-0 -n mongo 输出结果中VOLUME 那一列就是 PV 的名字。 123kubectl get pvc datadir-mongo-mongodb-sharded-shard0-data-0 -n mongoNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdatadir-mongo-mongodb-sharded-shard0-data-0 Bound pvc-82de37fd-b2c7-4b90-8b83-553da08114f0 100Gi RWO local-storage 16d 第三步：查看 PV 的物理路径 1kubectl get pv pvc-82de37fd-b2c7-4b90-8b83-553da08114f0 -o yaml spec 部分，查看路径信息。hostPath的path，就是物理地址了。来到这个路径，可以看到里面有.wt文件，这就是mongo存储数据的文件了。 从抽象需求（Pod&#x2F;PVC）到具体实现（PV&#x2F;物理路径）的逐层剥离： Pod 层 (mongo-shard0-data-0)： 视角： “我是数据库程序，我只知道把数据写到容器内的 &#x2F;bitnami&#x2F;mongodb&#x2F;data 目录。这个目录挂载了一个叫 datadir 的卷。” 线索： ClaimName: datadir-mongo-0 (我有这张提货券)。 PVC 层 (datadir-mongo-0)： 视角： “我是提货券，我要求 10GB 的空间。K8s 告诉我，我已经绑定到了 pvc-abcd-1234 这个资源上。” 线索： Volume: pvc-abcd-1234。 PV 层 (pvc-abcd-1234)： 视角： “我是真正的存储资源对象。我有详细的连接信息。我的类型是 HostPath。” 线索： Path: &#x2F;data&#x2F;k8s-volumes&#x2F;mongo&#x2F;shard0。 物理层 (Linux Server)： 视角： “我是硬盘上的一个文件夹。我就在 &#x2F;data&#x2F;k8s-volumes&#x2F;mongo&#x2F;shard0。” 为什么pod不能直接写死路径： 不可移植： 如果你的 Pod 迁移到了另一台服务器，或者换了一个集群，而新环境里没有 192.168.1.5 这台机器，Pod 就直接报错跑不起来了。 开发与运维分离： 开发人员（写 Pod 的人）通常不知道、也不应该关心服务器的 IP 和硬盘路径。运维人员（管 PV 的人）才需要关心。 分片架构为什么要这样设计核心理念只有四个字：**分而治之 (Divide and Conquer)**。 它的目的是为了解决单机性能瓶颈（磁盘不够大、CPU 跑满、内存不足）。为了实现无限的水平扩展（Horizontal Scaling），MongoDB 将功能拆解成了这三个独立的组件。 可以用一个“大型物流中心”的例子来完美类比这个架构： Mongos：前台接待&#x2F;调度员 (Router) 对应你的 Pod： mongo-mongodb-sharded-mongos-0 角色特点： 无状态、不存货、只动嘴。 为什么这么设计？ 它是应用程序（App）的唯一入口。你的代码只需要连接 mongos，就像你寄快递只找前台一样。 不存数据：因为它是负责计算路由逻辑的。它需要消耗 CPU 来解析你的查询命令，计算出数据在哪个分片上。 作用： 如果没有它，你的 App 就得自己去记“张三的数据在服务器 A，李四的数据在服务器 B”，这太痛苦了。Mongos 屏蔽了底层的复杂性。 Config Server：仓库账本&#x2F;目录管理员 (Metadata) 对应你的 Pod： mongo-mongodb-sharded-configsvr-0 角色特点： 存目录、数据量小、极度重要。 为什么这么设计？ 它存储的是**“元数据” (Metadata)**。也就是一张巨大的“地图”。 它记录了：“数据被切成了几块？如果在服务器 A 找不到，应该去服务器 B 找吗？哪个分片负责哪个范围的数据？” 关系： mongos 接到请求后，必须先问 Config Server：“嘿，用户要查 ID&#x3D;100 的数据，他在哪个仓库？”然后 mongos 才知道去哪里拿数据。 注意： 虽然它也存数据，但存的是配置信息，不是业务数据，所以它通常不需要很大的磁盘，但要求极高的数据一致性。 Shard (Data Node)：真正的仓库&#x2F;搬运工 (Data Storage) 对应你的 Pod： mongo-mongodb-sharded-shard0-data-0 角色特点： 存真货、干苦力、可无限增加。 为什么这么设计？ 这里存储了你真正的业务数据（比如用户表、订单表）。 分片 (Sharding) 的意思就是：假设你有 100TB 数据，不可能塞进一台机器。于是我们把数据切碎： Shard-0 存“A开头~H开头”的用户。 Shard-1 存“I开头~Z开头”的用户。 作用： 这就是为什么你要找数据必须看这个 Pod 的原因。所有的磁盘 I&#x2F;O 读写压力都在这里。 总结：一次查询的生命周期 为了串联它们，假设你的 K8s 集群里来了一个请求：“帮我查找用户 ID &#x3D; 5000 的资料”。 第一步（Mongos）： 请求到达 mongos-0。Mongos 只是个接待员，它手里没有数据。 第二步（Config）： Mongos 悄悄看了一眼缓存里的“账本”（这个账本是从 configsvr-0 同步过来的）。账本上写着：“ID 0~10000 的数据在 Shard-0 仓库里”。 第三步（Shard）： Mongos 转头把请求转发给 shard0-data-0。 第四步（返回）： shard0-data-0 在自己的硬盘里找到数据，返回给 Mongos，Mongos 再返回给用户。 为什么要拆得这么细？（设计优势） 无限扩展能力（Horizontal Scaling）： 如果数据把 shard0 塞满了怎么办？很简单，加一组 shard1 服务器。Mongos 会自动把一部分数据搬过去。你的 App 甚至感觉不到变化，不需要停机。 性能隔离： CPU 密集型任务（如路由计算、排序聚合）由 mongos 承担。 IO 密集型任务（读写磁盘）由 shard 承担。 如果查询太慢，可以单独给 mongos 加 CPU；如果磁盘不够，可以单独给 shard 加硬盘。 高可用（High Availability）： 在这个架构中，每一个组件（Shard, Config, Mongos）通常都是多副本的。 Shard 坏了一个节点？有备份节点顶上。 Mongos 坏了一个？K8s Service 会自动把流量切到另一个 Mongos 上。 这就是为什么你在 K8s 里看到这么多不同名字的 Pod，它们各司其职，共同支撑起了一个海量数据存储系统。"},{"title":"【mongoDB】聚合框架","path":"/2025/11/28/【mongoDB】聚合框架/","content":"常见操作1.常见管道阶段 分类 运算符 作用 &#x2F; 说明 常见场景 过滤 $match 按条件筛选文档，类似 find 的 query 只要上架商品、只要本月数据 投影&#x2F;加字段 $project &#x2F; $addFields &#x2F; $set 控制输出字段、重命名字段、增加计算字段 只返回必要字段、计算 finalPrice 分组统计 $group 按 _id 分组并做聚合（求和、计数、均值等） 按用户&#x2F;按天统计金额、数量 排序&#x2F;分页 $sort &#x2F; $skip &#x2F; $limit 排序、跳过 N 条、限制返回条数 列表页排序 + 分页 展开数组 $unwind 把数组里的每个元素“拆成多行” 订单里的 items 一条一条展开 关联集合 $lookup 类似左连接，把另一集合的数据查出来放到数组字段里 订单关联用户信息 多路统计 $facet 一次遍历，同时走多条子管道，返回多个统计结果 一次请求返回多种统计面板 分组计数排序 $sortByCount 对某个字段分组计数并按数量排序，相当于 $group + $sort 统计每个标签出现次数 计数 $count 统计当前管道流经文档总数，输出 { total: } 快速得到总数 一般常见使用顺序：原则：始终注意聚合管道的效率，确保限制文档数量。 1.匹配 $match | 相当于find，用来筛选符合条件的文档。 2.排序 $sort | 如果顺序很重要，一般会放在前面。 3.分页 $skip $limit | 表示从第几个开始，限制返回多少个。 4.投影 $project | 限制返回的字段。 例子： 123456789101112131415161718192021222324252627282930313233db.orders.aggregate([ // 只要本月订单 &#123; $match: &#123; createdAt: &#123; $gte: ISODate(&quot;2025-11-01T00:00:00Z&quot;), $lt: ISODate(&quot;2025-12-01T00:00:00Z&quot;) &#125; &#125; &#125;, // 按 userId 分组统计 &#123; $group: &#123; _id: &quot;$userId&quot;, orderCount: &#123; $sum: 1 &#125;, totalAmount: &#123; $sum: &quot;$amount&quot; &#125;, avgAmount: &#123; $avg: &quot;$amount&quot; &#125; &#125; &#125;, // 只要订单数 ≥ 3 的用户 &#123; $match: &#123; orderCount: &#123; $gte: 3 &#125; &#125; &#125;, // 输出格式美化 &#123; $project: &#123; _id: 0, userId: &quot;$_id&quot;, orderCount: 1, totalAmount: 1, avgAmount: 1 &#125; &#125;, &#123; $sort: &#123; totalAmount: -1 &#125; &#125;]) 2.常见聚合函数（累加器，主要在 $group 中） 类型 运算符 说明 统计类 $sum 求和；$sum: 1 时等价于计数 $avg 求平均值 $min &#x2F; $max 组内最小值 &#x2F; 最大值 $first &#x2F; $last 组内第一条 &#x2F; 最后一条文档的字段值（依赖排序） 收集类 $push 把每个值放进数组（允许重复） $addToSet 把不重复的值放进数组（自动去重） 3.常见表达式运算符（在 $project &#x2F; $addFields &#x2F; $group 等里用） 分类 运算符示例 说明 &#x2F; 用途 数学运算 $add &#x2F; $subtract &#x2F; $multiply &#x2F; $divide &#x2F; $mod 加减乘除、取余；如算含税价、差值等 字符串 $concat 拼接字符串（如 firstName + lastName） $substr &#x2F; $substrBytes &#x2F; $substrCP 截取字符串 $toUpper &#x2F; $toLower 大小写转换 日期 $year &#x2F; $month &#x2F; $dayOfMonth &#x2F; $hour 从日期中抽取年份、月份、日、小时等 $dateToString 格式化日期为字符串（如 YYYY-MM-DD） 条件判断 $cond if-else：condition ? then : else $ifNull 字段为 null&#x2F;不存在时给默认值 $switch 多分支条件判断 数组相关 $size 数组长度 $slice 取数组前 N 个元素 $in 判断某值是否在数组中"},{"title":"【算法】缓存淘汰算法LRU/LFU","path":"/2025/04/02/【算法】缓存淘汰算法LRU-LFU/","content":"无论是什么系统，在研发的过程中不可避免的会使用到缓存，而缓存一般来说我们不会永久存储，但是缓存的内容是有限的，那么我们如何在有限的内存空间中，尽可能的保留有效的缓存信息呢？ 那么我们就可以使用 LRU/LFU算法 ，来维持缓存中的信息的时效性。 LRU 详解原理 LRU （Least Recently Used：最近最少使用）算法在缓存写满的时候，会根据所有数据的访问记录，淘汰掉未来被访问几率最低的数据。也就是说该算法认为，最近被访问过的数据，在将来被访问的几率最大。 流程如下：假设我们有这么一块内存，一共有26个数据存储块。 当我们连续插入A、B、C、……、Z的时候，此时内存已经插满了 那么当我们再插入一个6，那么此时会将内存存放时间最久的数据A淘汰掉 当我们从外部读取数据C的时候，此时C就会提到头部，这时候C就是最晚淘汰的了 拆分一下的话，就是在维护一个双向链表 （当然在Java中就是用LinkedList，不过查找起来就比较麻烦，HashMap查找起来比较方便，即可以使用散列表来实现，ArrayList就要挪动太多元素了） 代码实现 定义一个存放的数据块结构 1234567type item struct &#123;\tkey string\tvalue any // the frequency of key (用来扩展LFU逻辑)\tfreq int&#125; 定义LRU算法的结构体 1234567type LRU struct &#123;\tdl *list.List // 维护的双端队列\tsize int // 当前的容量\tcapacity int // 限定的容量\tstorage map[string]*list.Element // 存储的key&#125; 获取某个key的value的函数，如果存在这个key，那么我们就把这个值移动到最前面MoveToFront，否则返回一个nil。 123456789func (c *LRU) Get(key string) any &#123;\tv, ok := c.storage[key]\tif ok &#123; c.dl.MoveToFront(v) return v.Value.(item).value\t&#125;\treturn nil&#125; 当我们需要put进去一些东西的时候。会分以下几个步骤 是否已经存在，如果已经存在则，直接返回，并且将key移动到最前面。 如果没有存在，但是已经是到极限容量了，就把最后一个Back()，淘汰掉，然后再塞入。 塞入的话，是塞入到最前面PushFront 123456789101112131415161718192021222324func (c *LRU) Put(key string, value any) &#123;\te, ok := c.storage[key]\tif ok &#123; n := e.Value.(item) n.value = value e.Value = n c.dl.MoveToFront(e) return\t&#125;\tif c.size &gt;= c.capacity &#123; e = c.dl.Back() dk := e.Value.(item).key c.dl.Remove(e) delete(c.storage, dk) c.size--\t&#125;\tn := item&#123;key: key, value: value&#125;\tc.dl.PushFront(n)\tne := c.dl.Front()\tc.storage[key] = ne\tc.size++&#125; 以上就是LRU算法的所有内容了，那我们看一下LFU算法。 LFU原理 LFU全称是最不经常使用算法（Least Frequently Used），LFU算法的基本思想和所有的缓存算法一样，一定时期内被访问次数最少的页，在将来被访问到的几率也是最小的。 相比于LRU（Least Recently Use）算法，LFU更加注重于使用的频率 。**LRU是其实可以看作是频率为1的LFU的。** 和LRU不同的是，LFU是根据频率排序的，当我们插入的时候，一般会把新插入的放到链表的尾部，因为新插入的一定是没有出现过的，所以频率都会是1 ， 所以会放在最后。 所以LFU的插入顺序如下： 如果A没有出现过，那么就会放在双向链表的最后，依次类推，就会是Z、Y。。C、B、A的顺序放到频率为1的链表中。 当我们新插入 A，B，C 那么A，B，C就会到频率为2的链表中 如果再次插入A，B那么A，B会在频率为3中。C依旧在2中 如果此时已经满了 ，新插入一个的话，我们会把最后一个D移除，并插入 6 代码定义一个LFU的结构体： 12345678910111213// LFU the Least Frequently Used (LFU) page-replacement algorithmtype LFU struct &#123;\tlen int // length\tcap int // capacity\tminFreq int // The element that operates least frequently in LFU\t// key: key of element, value: value of element\titemMap map[string]*list.Element\t// key: frequency of possible occurrences of all elements in the itemMap\t// value: elements with the same frequency\tfreqMap map[int]*list.List // 维护一个频率和list的集合&#125; 我们使用LFU算法的话，我们插入的元素就需要带上频率了 12345678// initItem to init item for LFUfunc initItem(k string, v any, f int) item &#123;\treturn item&#123; key: k, value: v, freq: f,\t&#125;&#125; 如果我们获取某个元素，那么这个元素如果存在，就会对这个元素的频率进行加1 12345678910111213// Get the key in cache by LFUfunc (c *LFU) Get(key string) any &#123;\t// if existed, will return value\tif e, ok := c.itemMap[key]; ok &#123; // the frequency of e +1 and change freqMap c.increaseFreq(e) obj := e.Value.(item) return obj.value\t&#125;\t// if not existed, return nil\treturn nil&#125; 增加频率 1234567891011121314// increaseFreq increase the frequency if elementfunc (c *LFU) increaseFreq(e *list.Element) &#123;\tobj := e.Value.(item)\t// remove from low frequency first\toldLost := c.freqMap[obj.freq]\toldLost.Remove(e)\t// change the value of minFreq\tif c.minFreq == obj.freq &amp;&amp; oldLost.Len() == 0 &#123; // if it is the last node of the minimum frequency that is removed c.minFreq++\t&#125;\t// add to high frequency list\tc.insertMap(obj)&#125; 插入key到LFU缓存中 如果存在就对频率加1 如果不存在就准备插入 如果溢出了，就把最少频率的删除 如果没有溢出，那么就放到最后 123456789101112131415161718192021222324// Put the key in LFU cachefunc (c *LFU) Put(key string, value any) &#123;\tif e, ok := c.itemMap[key]; ok &#123; // if key existed, update the value obj := e.Value.(item) obj.value = value c.increaseFreq(e)\t&#125; else &#123; // if key not existed obj := initItem(key, value, 1) // if the length of item gets to the top line // remove the least frequently operated element if c.len == c.cap &#123; c.eliminate() c.len-- &#125; // insert in freqMap and itemMap c.insertMap(obj) // change minFreq to 1 because insert the newest one c.minFreq = 1 // length++ c.len++\t&#125;&#125; 插入一个新的 123456789101112// insertMap insert item in mapfunc (c *LFU) insertMap(obj item) &#123;\t// add in freqMap\tl, ok := c.freqMap[obj.freq]\tif !ok &#123; l = list.New() c.freqMap[obj.freq] = l\t&#125;\te := l.PushFront(obj)\t// update or add the value of itemMap key to e\tc.itemMap[obj.key] = e&#125; 找到最少的链表，并且删除 123456789// eliminate clear the least frequently operated elementfunc (c *LFU) eliminate() &#123;\tl := c.freqMap[c.minFreq]\te := l.Back()\tobj := e.Value.(item)\tl.Remove(e)\tdelete(c.itemMap, obj.key)&#125; 以上就是所有LFU的算法实现了。 —在研究如何识别热key时发现，阿里是在代理层，借助LFU写的，所以说这玩意儿还是有用啊，我学学学！ 参考资料： 图解缓存淘汰算法 LRU、LFU ｜ 最近最少使用、最不经常使用算法 ｜ go语言实现 不该这么嚣张的，B站面试官水平真高，手写LRU算法失算了"},{"title":"【系统设计】如何定位Redis热key","path":"/2025/04/02/【系统设计】如何定位Redis热key/","content":"热key指一段时间内被频繁访问或操作的键。通常出现在商品限时抢购、瞬时新闻热点等业务场景，可能会对系统的稳定性和可用性造成影响，比如对应节点的网卡带宽被打满，出现丢包重传，请求波动耗时大幅上升，甚至影响到业务的正常使用，引发用户的不满。因此，不可能等到热Key出现已经拖垮了服务再去处理，那个时候业务一定已经受到影响，因此需要提前尽可能在设计和开发时避免引入全局热key，另外，真实的生成环境还是可能存在边界case、非预期的流量等，因此快速定位热key仍然需要研究。 需要做到高并发和高性能，对于高频变化的动态数据通常采用多级缓存，搭配缓存预热、缓存失效时长等手段控制。 按照实现的原理，我们可以尝试利用内置的命令或工具，或者外部流量工具，甚至代码层面开统计key的访问频率，进而对热key做进一步操作。 一、探测思路从Redis请求路径的节点入手。 1. 客户端收集上报改动 Redis SDK，记录每个请求，定时把收集到的数据上报，然后由一个统一的服务进行聚合计算。 优点：方案直观简单 缺点：没法适应多语言架构，一方面多语言 SDK 对齐是个问题，另外一方面后期 SDK 的维护升级会面临比较大的困难，成本很高 2. 代理层收集上报如果所有的 Redis 请求都经过代理的话，可以考虑改动 Proxy 代码进行收集，思路与客户端基本类似。 优点：对使用方完全透明，能够解决客户端 SDK 的语言异构和版本升级问题 缺点：开发成本会比客户端高些 3. Redis 数据定时扫描利用Redis自带的工具，优点：无需进行二次开发，能够直接利用现成的工具。 （1）--hotkeys 参数 Redis在4.0版本后提供了hotkeys功能，可以通过redis-cli --hotkeys命令获取当前keyspace的热点key，实现上是通过 scan + object freq 完成的。 缺点： 由于需要扫描整个 keyspace，实时性上比较差; 扫描时间与 key 的数量正相关，如果 key 的数量比较多，耗时可能会非常长。 （2） monitor命令 通过 redis-cli monitor实时抓取出 Redis 服务器接收到的命令，同时结合一些现成的分析工具，比如 redis-faina，统计出热 Key。 缺点：在高并发的条件下，有内存暴增的隐患，还会降低 Redis 的性能。 （3）慢查询日志 使用redis-cli slowlog命令查看慢查询日志，记录执行时间超过阈值的命令。间接发现热Key，侧重慢查询，非高频访问。 4. Redis 节点抓包解析在可能存在热 key 的节点上(流量倾斜判断)，通过 tcpdump 抓取一段时间内的流量并上报，然后由一个外部的程序进行解析、聚合和计算。 优点：该方案无需侵入现有的 SDK 或者 Proxy 中间件，开发维护成本可控 缺点：热 key 节点的网络流量和系统负载已经比较高了，抓包可能会情况进一步恶化 二、热key的治理三、生产案例阿里第二层，代理层上报 京东 Jdhotkey解决的是广义的热key问题，不属于探测思路章节中四种中的任何一种，不修改Redis SDK、不依赖Proxy、不扫描Redis数据、也不抓包。它通过独立的ETCD+Worker集群实现热key探测，与Redis无耦合。 相关补充： DAU（Daily Active Users，日活跃用户数），指一天内（24小时内）使用产品或服务的独立用户数量，是衡量产品活跃度和用户粘性的重要指标之一。具体由产品的体量决定，例如20w的DAU，对于淘宝就很小，对于小众论坛就很大。 补充：在Windows如何启动redis？ 进入安装目录并cmd 运行命令redis-server.exe redis.windows.conf。如果报错，依次执行第一条指令：redis-cli.exe，第二条指令：shutdown，第三条指令：exit 参考资料： Redis 热 Key 发现以及解决办法 如何快速定位 Redis 热 key（上）-阿里云开发者社区 工具推荐： Redis-faina:facebookarchive&#x2F;redis-faina: A query analyzer that parses Redis’ MONITOR command for counter&#x2F;timing stats about query patterns (github.com) 京东热key探测框架：京东毫秒级热key探测框架设计与实践，已实战于618大促"},{"title":"【系统设计】秒杀系统实现方案与技术","path":"/2025/03/28/【系统设计】秒杀系统实现方案与技术/","content":"秒杀系统作为互联网“高并发、高性能、高可用”系统的代表，从系统设计、数据处理到运维保障等方面都有很多可以考察和深挖的点，本文将尝试分析涉及的关键问题，并总结相关的最佳实践。 秒杀系统的核心挑战在于平衡性能、一致性与安全性。 秒杀系统中常见的问题包括：超卖问题、高并发性能瓶颈、数据一致性问题、分布式锁失效、事务管理失效、安全与放作弊问题、系统监控与运维等。 通过分层架构（如流量削峰、异步化）、原子操作（如Lua脚本）、分布式协调（如Redis锁）及全周期管理（监控+应急预案）综合解决。设计时应优先保障核心链路（如库存扣减），非核心功能（如积分赠送）允许降级或最终一致性。 从“高并发、高性能、高可用”来看高并发 限流 流量削峰漏斗：业务校验分层过滤（如，账号安全、购买资格-&gt;系统基本信息-&gt;商品信息-&gt;秒杀时间-&gt;数据库写操作） 接口限流：借助现有组件，如Sentinel 问题、验证码：避免请求集中、解决脚本作弊等问题（如，对正确性校验、对提交时间校验） 提前预约：提前筛选黄牛 高性能也关于高并发 热点数据隔离：分为静态和动态 静态数据基本不变，例如商品信息等：利用CDN内容分发服务器 动态数据高频变化，例如库存等： 多级缓存 缓存预热 缓存失效时长控制 本地缓存LocalCache 少量极热数据可以放在JVM 涉及另一个问题：识别热点key 中间件层面：如京东零售的hotkey 高可用 集群化 流量削峰：借助消息队列 降级：优先保证核心业务（应对系统自身的故障） 熔断：中断对该服务的调用（应对系统依赖外部系统或第三方的故障） 从数据一致性来看扣减库存方案首先常见的方案包括下单扣减库存（常用）和付款扣减库存，另外超时不付款则释放库存。 一个很常见的是超卖问题。其解决方案： 提前把商品信息放到缓存（考虑分布式缓存） 通过lua脚本操作Redis，执行原子性操作 数据库唯一键 余额扣减方案 并发高：悲观锁 数据库的排他锁 并发不高：乐观锁 版本号机制 （注意ABA问题） 系统架构设计以阿里电影节为例 架构设计上，介入统一网关进行安全保障和限流，库存和订单的数据隔离，加入多级缓存： 业务设计上，对外业务和后台管理可以分开。内网的业务服务、基础服务和中间件分开： 其他针对恶意操作 库存失效时间 限制用户购买数量 黄牛防控系统 性能测试 常用工具 Jmeter LoadRunner Galtling ab 其他： 精准监控 数据大盘 接口幂等 状态机 分布式锁 —作为项目挖掘机的记录……待补充完善……"},{"title":"【数据库】如何保障数据库和缓存一致性","path":"/2025/03/27/【数据库】如何保障数据库和缓存一致性/","content":"数据库和缓存的一致性问题，在面试以及实践中都是非常重要的知识点，而一般面试者只能说出最佳的实践是什么（即延迟双删或者先更新数据库再删除缓存key），但是不能通过线程之间的读写关系举例说明为什么要这样实践，本文通过穷尽更新缓存的四种方式进行分析，得出了这个结论。最后，本文还介绍了利用消息中间件MQ应对其他更复杂的情形。 [建议先看思维导图和How的总结] Why缓存？ 缓存合理使用确提升了系统的吞吐量和稳定性，然而这是有代价的。这个代价便是缓存和数据库的一致性带来了挑战，本文将针对最常见的 cache-aside 策略下如何维护缓存一致性彻底讲透。 在真实的业务场景中，我们的业务的数据——例如订单、会员、支付等——都是持久化到数据库中的，因为数据库能有很好的事务保证、持久化保证。但是，正因为数据库要能够满足这么多优秀的功能特性，使得数据库在设计上通常难以兼顾到性能，因此往往不能满足大型流量下的性能要求，像是 MySQL 数据库只能承担“千”这个级别的 QPS，否则很可能会不稳定，进而导致整个系统的故障。 但是客观上，我们的业务规模很可能要求着更高的 QPS，有些业务的规模本身就非常大，也有些业务会遇到一些流量高峰，比如电商会遇到大促的情况。 而这时候大部分的流量实际上都是读请求，而且大部分数据也是没有那么多变化的，如热门商品信息、微博的内容等常见数据就是如此。此时，缓存就是我们应对此类场景的利器。 所谓缓存，实际上就是用空间换时间，准确地说是用更高速的空间来换时间，从而整体上提升读的性能。 何为更高速的空间呢？ 更快的存储介质。通常情况下，如果说数据库的速度慢，就得用更快的存储组件去替代它，目前最常见的就是 Redis（内存存储）。Redis 单实例的读 QPS 可以高达 10w&#x2F;s，90% 的场景下只需要正确使用 Redis 就能应对。 就近使用本地内存。就像 CPU 也有高速缓存一样，缓存也可以分为一级缓存、二级缓存。即便 Redis 本身性能已经足够高了，但访问一次 Redis 毕竟也需要一次网络 IO，而使用本地内存无疑有更快的速度。不过单机的内存是十分有限的，所以这种一级缓存只能存储非常少量的数据，通常是最热点的那些 key 对应的数据。这就相当于额外消耗宝贵的服务内存去换取高速的读取性能。 Challenges？引入缓存后的一致性挑战 用空间换时间，意味着数据同时存在于多个空间。最常见的场景就是数据同时存在于 Redis 与 MySQL 上（为了问题的普适性，后面举例中若没有特别说明，缓存均指 Redis 缓存）。 实际上，最权威最全的数据还是在 MySQL 里的。而万一 Redis 数据没有得到及时的更新（例如数据库更新了没更新到 Redis），就出现了数据不一致。 大部分情况下，只要使用了缓存，就必然会有不一致的情况出现，只是说这个不一致的时间窗口是否能做到足够的小。有些不合理的设计可能会导致数据持续不一致，这是我们需要改善设计去避免的。 这里的一致性实际上对于本地缓存也是同理的，例如数据库更新后没有及时更新本地缓存，也是有一致性问题的，下文统一以 Redis 缓存作为引子讲述，实际上处理本地缓存原理基本一致。 缓存不一致性无法客观地完全消灭 为什么我们几乎没办法做到缓存和数据库之间的强一致呢？ 理想情况下，我们需要在数据库更新完后把对应的最新数据同步到缓存中，以便在读请求的时候能读到新的数据而不是旧的数据（脏数据）。但是很可惜，由于数据库和 Redis 之间是没有事务保证的，所以我们无法确保写入数据库成功后，写入 Redis 也是一定成功的；即便 Redis 写入能成功，在数据库写入成功后到 Redis 写入成功前的这段时间里，Redis 数据也肯定是和 MySQL 不一致的。如下两图所示： 无法事务保持一致 所以说这个时间窗口是没办法完全消灭的，除非我们付出极大的代价，使用分布式事务等各种手段去维持强一致，但是这样会使得系统的整体性能大幅度下降，甚至比不用缓存还慢，这样不就与我们使用缓存的目标背道而驰了吗？ 不过虽然无法做到强一致，但是我们能做到的是缓存与数据库达到最终一致，而且不一致的时间窗口我们能做到尽可能短，按照经验来说，如果能将时间优化到 1ms 之内，这个一致性问题带来的影响我们就可以忽略不计。 最终一致性如何保证？–&gt;缓存设置过期时间 第一个方法便是我们上面提到的，当我们无法确定 MySQL 更新完成后，缓存的更新&#x2F;删除一定能成功，例如 Redis 挂了导致写入失败了，或者当时网络出现故障，更常见的是服务当时刚好发生重启了，没有执行这一步的代码。 这些时候 MySQL 的数据就无法刷到 Redis 了。为了避免这种不一致性永久存在，使用缓存的时候，我们必须要给缓存设置一个过期时间，例如 1 分钟，这样即使出现了更新 Redis 失败的极端场景，不一致的时间窗口最多也只是 1 分钟。 这是我们最终一致性的兜底方案，万一出现任何情况的不一致问题，最后都能通过缓存失效后重新查询数据库，然后回写到缓存，来做到缓存与数据库的最终一致。 How更新缓存?引言 通常情况下，我们在处理查询请求的时候，使用缓存的逻辑如下： 1234567data = queryDataRedis(key);if (data ==null) &#123; data = queryDataMySQL(key); //缓存查询不到，从MySQL做查询 if (data!=null) &#123; updateRedis(key, data);//查询完数据后更新MySQL最新数据到Redis &#125;&#125; 也就是说优先查询缓存，查询不到才查询数据库。如果这时候数据库查到数据了，就将缓存的数据进行更新。这是我们常说的 cache aside 的策略，也是最常用的策略。 这样的逻辑是正确的，而一致性的问题一般不来源于此，而是出现在处理写请求的时候。所以我们简化成最简单的写请求的逻辑，此时你可能会面临多个选择，究竟是直接更新缓存，还是失效缓存？而无论是更新缓存还是失效缓存，都可以选择在更新数据库之前，还是之后操作。 这样就演变出 4 个策略：更新数据库后更新缓存、更新数据库前更新缓存、更新数据库后删除缓存、更新数据库前删除缓存。下面我们来分别讲述。 1. 更新数据库后更新缓存的不一致问题一种常见的操作是，设置一个过期时间，让写请求以数据库为准，过期后，读请求同步数据库中的最新数据给缓存。那么在加入了过期时间后，是否就不会有问题了呢？并不是这样。 大家设想一下这样的场景。 假如这里有一个计数器，把数据库自减 1，原始数据库数据是 100，同时有两个写请求申请计数减一，假设线程 A 先减数据库成功，线程 B 后减数据库成功。那么这时候数据库的值是 98，缓存里正确的值应该也要是 98。 但是特殊场景下，你可能会遇到这样的情况： 线程 A 和线程 B 同时更新这个数据。 更新数据库的顺序是先 A 后 B。 更新缓存时顺序是先 B 后 A。 如果我们的代码逻辑还是更新数据库后立刻更新缓存的数据，那么—— 12updateMySQL();updateRedis(key, data); 就可能出现：数据库的值是 100-&gt;99-&gt;98，但是缓存的数据却是 100-&gt;98-&gt;99，也就是数据库与缓存的不一致。而且这个不一致只能等到下一次数据库更新或者缓存失效才可能修复。 时间线程A（写请求）线程B（写请求）问题T1更新数据库为99T2更新数据库为98T3更新缓存数据为98T4更新缓存数据为99此时缓存的值被显式更新为99，但是实际上数据库的值已经是98，数据不一致 当然，如果更新 Redis 本身是失败的话，两边的值固然也是不一致的，这个前文也阐述过，几乎无法根除。 2. 更新数据库前更新缓存的不一致问题那你可能会想，这是否表示，我应该先让缓存更新，之后再去更新数据库呢？类似这样： 12updateRedis(key, data);//先更新缓存updateMySQL();//再更新数据库 这样操作产生的问题更是显而易见的，因为我们无法保证数据库的更新成功，万一数据库更新失败了，你缓存的数据就不只是脏数据，而是错误数据了。 你可能会想，是否我在更新数据库失败的时候做 Redis 回滚的操作能够解决呢？这其实也是不靠谱的，因为我们也不能保证这个回滚的操作 100% 被成功执行。 同时，在写写并发的场景下，同样有类似的一致性问题，请看以下情况： 线程 A 和线程 B 同时更新同这个数据。 更新缓存的顺序是先 A 后 B。 更新数据库的顺序是先 B 后 A。 举个例子。线程 A 希望把计数器置为 0，线程 B 希望置为 1。而按照以上场景，缓存确实被设置为 1，但数据库却被设置为 0。 所以通常情况下，更新缓存再更新数据库是我们应该避免使用的一种手段。 3. 更新数据库前删除缓存的问题那如果采取删除缓存的策略呢？也就是说我们在更新数据库的时候失效对应的缓存，让缓存在下次触发读请求时进行更新，是否会更好呢？同样地，针对在更新数据库前和数据库后这两个删除时机，我们来比较下其差异。 最直观的做法，我们可能会先让缓存失效，然后去更新数据库，代码逻辑如下： 12deleteRedis(key);//先删除缓存让缓存失效updateMySQL();//再更新数据库 这样的逻辑看似没有问题，毕竟删除缓存后即便数据库更新失败了，也只是缓存上没有数据而已。然后并发两个写请求过来，无论怎么样的执行顺序，缓存最后的值也都是会被删除的，也就是说在并发写写的请求下这样的处理是没问题的。 然而，这种处理在读写并发的场景下却存在着隐患。 还是刚刚更新计数的例子。例如现在缓存的数据是 100，数据库也是 100，这时候需要对此计数减 1，减成功后，数据库应该是 99。如果这之后触发读请求，缓存如果有效的话，里面应该也要被更新为 99 才是正确的。 那么思考下这样的请求情况： 线程 A 更新这个数据的同时，线程 B 读取这个数据。 线程 A 成功删除了缓存里的老数据，这时候线程 B 查询数据发现缓存失效。 线程 A 更新数据库成功。 时间线程A（写请求）线程B（读请求）问题T1删除缓存值T21.读取缓存数据，缓存缺失，从数据库读取数据100T3更新数据库中的数据X的值为99T4将数据100的值写入缓存此时缓存的值被显式更新为100，但是实际上数据库的值已经是99了 可以看到，在读写并发的场景下，一样会有不一致的问题。 针对这种场景，有个做法是所谓的“延迟双删策略”，就是说，既然可能因为读请求把一个旧的值又写回去，那么我在写请求处理完之后，等到差不多的时间延迟再重新删除这个缓存值。 时间线程A（写请求）线程C（新的读请求）线程D（新的读请求）问题T5sleep(N)缓存存在，读取到缓存旧值100其他线程可能在双删成功前读到脏数据T6删除缓存值T7缓存缺失，从数据库读取数据的最新值（99） 这种解决思路的关键在于对 N 的时间的判断，如果 N 时间太短，线程 A 第二次删除缓存的时间依旧早于线程 B 把脏数据写回缓存的时间，那么相当于做了无用功。而 N 如果设置得太长，那么在触发双删之前，新请求看到的都是脏数据。 4. 更新数据库后删除缓存那如果我们把更新数据库放在删除缓存之前呢，问题是否解决？我们继续从读写并发的场景看下去，有没有类似的问题。 时间线程A（写请求）线程B（读请求）线程C（读请求）潜在问题T1更新主库 X = 99（原值 X = 100）T2读取数据，查询到缓存还有数据，返回100线程C实际上读取到了和数据库不一致的数据T3删除缓存T4查询缓存，缓存缺失，查询数据库得到当前值99T5将99写入缓存 可以看到，大体上，采取先更新数据库再删除缓存的策略是没有问题的，仅在更新数据库成功到缓存删除之间的时间差内——[T2,T3)的窗口 ，可能会被别的线程读取到老值。 而在开篇的时候我们说过，缓存不一致性的问题无法在客观上完全消灭，因为我们无法保证数据库和缓存的操作是一个事务里的，而我们能做到的只是尽量缩短不一致的时间窗口。 在更新数据库后删除缓存这个场景下，不一致窗口仅仅是 T2 到 T3 的时间，内网状态下通常不过 1ms，在大部分业务场景下我们都可以忽略不计。因为大部分情况下一个用户的请求很难能再 1ms 内快速发起第二次。 但是真实场景下，还是会有一个情况存在不一致的可能性，这个场景是读线程发现缓存不存在，于是读写并发时，读线程回写进去老值。并发情况如下： 时间线程A（写请求）线程B（读请求--缓存不存在场景）潜在问题T1查询缓存，缓存缺失，查询数据库得到当前值100T2更新主库 X = 99（原值 X = 100）T3删除缓存T4将100写入缓存此时缓存的值被显式更新为100，但是实际上数据库的值已经是99了 总的来说，这个不一致场景出现条件非常严格，因为并发量很大时，缓存不太可能不存在；如果并发很大，而缓存真的不存在，那么很可能是这时的写场景很多，因为写场景会删除缓存。 所以待会我们会提到，写场景很多时候实际上并不适合采取删除策略。 总结 【总结】红字为相应的解决方案，但是这些方案或多或少都存在一些问题： 分布式锁：操作重 用MQ确认：复杂 延迟双删：关键在于sleep(N)的N 太短：早于新的读请求，于是新的读请求请求了数据库又往缓存写入了脏数据，无用功 太长：新的读请求都得到了脏数据 【总结】最佳实践：更新数据库后删除缓存值 读多写少–&gt;更新数据库后删除缓存 读写相当&#x2F;写多读少–&gt;更新数据库后更新缓存 【原总结，看完上面的还没理解可以往下看】终上所述，我们对比了四个更新缓存的手段，做一个总结对比，其中应对方案也提供参考，具体不做展开，如下表： 策略并发场景潜在问题应对方案更新数据库+更新缓存写+读线程A未更新完缓存之前，线程B的读请求会短暂读到旧值可以忽略写+写更新数据库的顺序是先A后B，但更新缓存时顺序是先B后A，数据库和缓存数据不一致分布式锁（操作重）更新缓存+更新数据库无并发线程A还未更新完缓存但是更新数据库可能失败利用MQ确认数据库更新成功（较复杂）写+写更新缓存的顺序是先A后B，但更新数据库时顺序是先B后A分布式锁（操作很重）删除缓存值+更新数据库写+读写请求的线程A删除了缓存在更新数据库之前，这时候读请求线程B到来，因为缓存缺失，则把当前数据读取出来放到缓存，而后线程A更新成功了数据库延迟双删（但是延迟的时间不好估计，且延迟的过程中依旧有不一致的时间窗口）更新数据库+删除缓存值写+读（缓存命中）线程A完成数据库更新成功后，尚未删除缓存，线程B有并发读请求会读到旧的脏数据可以忽略写+读（缓存不命中）读请求不命中缓存，写请求处理完之后读请求才回写缓存，此时缓存不一致分布式锁（操作重） 从一致性的角度来看，采取更新数据库后删除缓存值，是更为适合的策略。因为出现不一致的场景的条件更为苛刻，概率相比其他方案更低。 那么是否更新缓存这个策略就一无是处呢？不是的！ 删除缓存值意味着对应的 Key 会失效，那么这时候读请求都会打到数据库。如果这个数据的写操作非常频繁，就会导致缓存的作用变得非常小。而如果这时候某些 Key 还是非常大的热 Key，就可能因为扛不住数据量而导致系统不可用。 如下图所示： 删除策略频繁的缓存失效导致读请求无法利用缓存 所以做个简单总结，足以适应绝大部分的互联网开发场景的决策： 针对大部分读多写少场景，建议选择更新数据库后删除缓存的策略。 针对读写相当或者写多读少的场景，建议选择更新数据库后更新缓存的策略。 Addition:消息中间件的运用 如何减少缓存删除&#x2F;更新的失败？万一删除缓存这一步因为服务重启没有执行，或者 Redis 临时不可用导致删除缓存失败了，就会有一个较长的时间（缓存的剩余过期时间）是数据不一致的。 那我们有没有什么手段来减少这种不一致的情况出现呢？这时候借助一个可靠的消息中间件就是一个不错的选择。 因为消息中间件有 ATLEAST-ONCE 的机制，如下图所示。 我们把删除 Redis 的请求以消费 MQ 消息的手段去失效对应的 Key 值，如果 Redis 真的存在异常导致无法删除成功，我们依旧可以依靠 MQ 的重试机制来让最终 Redis 对应的 Key 失效。 而你们或许会问，极端场景下，是否存在更新数据库后 MQ 消息没发送成功，或者没机会发送出去机器就重启的情况？ 这个场景的确比较麻烦，如果 MQ 使用的是 RocketMQ，我们可以借助 RocketMQ 的事务消息，来让删除缓存的消息最终一定发送出去。而如果你没有使用 RocketMQ，或者你使用的消息中间件并没有事务消息的特性，则可以采取消息表的方式让更新数据库和发送消息一起成功。事实上这个话题比较大了，我们不在这里展开。 如何处理复杂的多缓存场景？有些时候，真实的缓存场景并不是数据库中的一个记录对应一个 Key 这么简单，有可能一个数据库记录的更新会牵扯到多个 Key 的更新。还有另外一个场景是，更新不同的数据库的记录时可能需要更新同一个 Key 值，这常见于一些 App 首页数据的缓存。 我们以一个数据库记录对应多个 Key 的场景来举例。 假如系统设计上我们缓存了一个粉丝的主页信息、主播打赏榜 TOP10 的粉丝、单日 TOP 100 的粉丝等多个信息。如果这个粉丝注销了，或者这个粉丝触发了打赏的行为，上面多个 Key 可能都需要更新。只是一个打赏的记录，你可能就要做： 1234updateMySQL();//更新数据库一条记录deleteRedisKey1();//失效主页信息的缓存updateRedisKey2();//更新打赏榜TOP10deleteRedisKey3();//更新单日打赏榜TOP100 这就涉及多个 Redis 的操作，每一步都可能失败，影响到后面的更新。甚至从系统设计上，更新数据库可能是单独的一个服务，而这几个不同的 Key 的缓存维护却在不同的 3 个微服务中，这就大大增加了系统的复杂度和提高了缓存操作失败的可能性。最可怕的是，操作更新记录的地方很大概率不只在一个业务逻辑中，而是散发在系统各个零散的位置。 针对这个场景，解决方案和上文提到的保证最终一致性的操作一样，就是把更新缓存的操作以 MQ 消息的方式发送出去，由不同的系统或者专门的一个系统进行订阅，而做聚合的操作。如下图： 不同业务系统订阅 MQ 消息单独维护各自的缓存 Key 通过订阅 MySQL binlog 的方式处理缓存上面讲到的 MQ 处理方式需要业务代码里面显式地发送 MQ 消息。还有一种优雅的方式便是订阅 MySQL 的 binlog，监听数据的真实变化情况以处理相关的缓存。 例如刚刚提到的例子中，如果粉丝又触发打赏了，这时候我们利用 binlog 表监听是能及时发现的，发现后就能集中处理了，而且无论是在什么系统什么位置去更新数据，都能做到集中处理。 目前业界类似的产品有 Canal，具体的操作图如下： 利用 Canel 订阅数据库 binlog 变更从而发出 MQ 消息，让一个专门消费者服务维护所有相关 Key 的缓存操作 到这里，针对大型系统缓存设计如何保证最终一致性，我们已经从策略、场景、操作方案等角度进行了细致的讲述，希望能对你起到帮助。 参考资料：一文讲透数据库缓存一致性问题 (qq.com) 扩展学习： 为什么Redis快？ 消息中间件的ATLEAST-ONCE是什么机制？ 如何更新数据库和发送MQ一起成功，事务消息是什么？消息表又是什么？ 数据库有哪些log？分别是什么作用？"},{"title":"【Kubernetes】如何搭建Kubenetes集群","path":"/2025/03/24/【Kubernetes】如何搭建Kubenetes集群/","content":"本文将使用kubeadm模式快速部署一主两从集群。 虚拟机准备首先本地需要准备：CentOS7.x-86_x64镜像，硬件至少2GB。 然后打开VMware WorkStation新建三个虚拟机（新建虚拟机教程，可以选择基础设施服务器），分别命名为master、node1、node2。 接下来确保虚拟机能够访问外网（注意宿主机不要连校园网），采用NAT模式，操作如下： 检查宿主机的适配器VMnet8的ip网段：cmd中执行ipconfig，VMnet8的ip要和外网处于同一网段 如果不在同一个网段，则手动配置VMnet8的ip和掩码 检查虚拟机：虚拟网络编辑器的NAT设置是否网段一致，手动配置 修改/etc/sysconfig/network-scripts/ifcfg-ens33文件，配置虚拟机网断、掩码、网关、DNS 使之生效：systemctl restart network 重启网络服务 （如果failed看解决 Linux 网络 “Job for network.service failed because the control process exite”） ping网关 192.168.43.2可以，ping百度 www.baidu.com可以，完成！ 系统准备"},{"title":"【Go基础】分布式事务","path":"/2025/03/23/【Go基础】分布式事务/","content":"从几个缩写讲起首先，提到事务，一般指的是数据库的事务，指逻辑上的一组操作，要么都执行，要么都不执行。 ACID，指的是数据库在写入或者更新资料时，为了保证交易正确可靠，要具备的4个特性： 缩写 英文单词 中文解释 说明 A atomicity 原子性 最小执行单位，all or nothing C consistency 一致性 执行前后一致 I isolation 隔离性 并发时，事务间不干扰 D durability 持久性 持久改变 这里要特别注意，C一致性是最终的目的，其余三个是实现C的手段。在单机上实现ACID可以通过锁、时间序列等机制。 接下来是分布式事务，与微服务密切相关，因为不同的微服务一般会使用自己的数据库，这个时候要满足ACID就比较困难了，如何保证系统中多个相关联的数据库中的数据一致？ 此时，需要选择折中的方案，为此，引进了CAP理论： 缩写 英文单词 中文解释 说明 C consistency 一致性 所有节点访问同份最新数据副本，要么返回最新数据要么失败 A availability 可用性 非故障节点在合理时间返回合理响应，不保证数据一致 P partition tolerance 分区容忍性 出现网络分区时仍对外提供服务 分布式系统必须保障能够对外提供服务，即分区容错性是必须的。不可能三角指的是在读写操作时，假设出现了网络分区，只能满足两个，即CP或者AP。这里要特别注意，如果没有出现网络分区，A和C是可以同时满足的。当数据不一致会影响业务时，选择CP，当业务需要高可用时，选择AP。常用的注册中心中，Zookeeper保证了CP，Eureka保证了AP，Nacos二者都支持。 在C和A的权衡实践中，诞生了BASE理论： 缩写 英文单词 中文解释 说明 B basically available 基本可用性 允许损失部分可用性（响应时间延长，损失部分非核心功能等） A availability 可用性 S soft-state 软状态 允许数据不一致，不影响整体可用性 E eventually consistent 最终一致性 一致的三个级别 这里需要理清一致性的3个级别： 强一致性：在银行等场景需要保证； 弱一致性：什么时候达到一致的状态完全没有保证（所以基本不用）； 最终一致性：系统保证在一定时间内达到一致，业界比较推崇，那么如何保证最终一致性： 读时修复 写时修复（性能好） 定期修复（常用） 分布式事务的解决方案在分布式系统中，如何保障各个节点之间的ACID特性？主要解决方案可分为两大类： 1. 强一致性方案 二阶段提交协议（2PC） 三阶段提交协议（3PC） 2. 最终一致性方案 补偿事务（TCC） MQ事务 Saga事务 本地消息表 其中： 2PC、3PC 属于业务代码无侵入方案，基于 XA 规范衍生而来。 TCC、Saga 属于业务入侵方案，需要开发者手动实现补偿逻辑。 MQ 事务依赖于消息队列，本地消息表不支持回滚。 强一致性方案（2PC &amp; 3PC）XA规范根据XA规范设计，首先介绍XA规范涉及的角色： AP（Application Program）：应用程序 RM（Resource Manager）：资源管理器（通常指数据库，也有文件系统、MQ系统），提供操作数据的接口等，保证数据一致和完整 TM（Transaction Manager）：事务管理器，是一个协调者的角色，协调跨库事务关联的所有RM的行为 2PC（两阶段提交） 准备阶段（Prepare） TM 记录事务开始日志，询问所有 RM 是否可以执行提交准备操作。 RM 尝试执行本地事务的预备操作：锁定资源，执行事务但不提交。[if 失败]则告知TM，并回滚自己的操作，不参与本次事务。 TM 收集RM的响应，记录事务准备完成日志。 提交阶段（Commit） 若所有 RM 均准备成功，TM 通知 RM 提交事务。 若有失败，则 TM 让所有 RM 回滚事务。 问题： 阻塞问题：等待提交时，资源被锁定。 单点故障：如果 TM 宕机，可能导致事务卡住。 3PC（三阶段提交）对 2PC 进行了改进，增加了超时机制。 CanCommit（准备阶段） TM 询问 RM 是否可提交事务。 RM 返回 Yes &#x2F; No &#x2F; 超时。 PreCommit（预提交阶段） 若所有 RM 均返回 Yes，TM 发送预提交请求。 RM 预执行事务，等待最终确认。 若有 RM 失败或超时，则 TM 发送中断请求。 DoCommit（提交阶段） 若所有 RM 均完成预提交，TM 发送最终提交请求。 进入该阶段后，基本不会失败。 改进点： 增加超时机制，避免事务永久阻塞。 通过 CanCommit 阶段减少资源长时间锁定。但是解决并不完美，性能差、数据仍然不一致，应用不广泛，一般会通过复制状态机解决2PC的阻塞问题。 最终一致性方案（TCC &amp; Saga &amp; MQ &amp; 本地消息表）TCC（Try-Confirm-Cancel）适用于高并发、低延迟的业务场景，例如支付系统。 Try（尝试执行）：进行业务检查，预留资源。 Confirm（确认执行）：若所有 Try 操作成功，则正式提交。 Cancel（取消执行）：若某个 Try 失败，则执行回滚操作。 注意： 需要业务开发者自己实现 Try、Confirm、Cancel 逻辑。 Confirm 失败时一般会重试，最终仍失败则需人工介入。 MQ 事务基于 两阶段提交，适用于 事件驱动架构。 发送半消息，等待本地事务执行。 本地事务执行成功，则确认发送消息；失败则回滚消息。 事务反查机制：检查消息是否成功发送。 消息消费失败时，消息队列会自动进行重试，超过最大次数进入 死信队列，等待人工处理。 特点： 适用于 跨系统事务（如支付完成后通知订单系统）。 异步处理，吞吐量高，但不保证强一致性。 Saga 事务适用于 长事务（如电商订单流程：支付 → 发货 → 确认收货）。 将事务拆分为 多个子事务，每个子事务执行完即提交。 若某个子事务失败，则触发 补偿事务 进行回滚。 Saga 事务没有预留资源，不保证隔离性。 Seata 是典型的 Saga 事务实现。 问题： 需要业务开发者自己编写补偿逻辑。 若补偿失败，则需人工介入。 本地消息表适用于 保证可靠消息的场景。 事务执行时，先写入数据库本地消息表。 定时扫描消息表，将消息投递到 MQ。 缺点：不支持事务回滚，需额外补偿机制。 方案对比总结 方案 业务代码侵入性 适用场景 关键特性 2PC 无侵入 传统数据库事务 强一致性，阻塞问题 3PC 无侵入 分布式数据库事务 改进 2PC，仍有不一致风险 TCC 需要开发者实现 高并发业务，如金融支付 高性能，需要 Try-Confirm-Cancel Saga 需要开发者实现 长事务，如订单流程 适用于多步骤事务，无隔离性 MQ 事务 依赖 MQ 事件驱动架构 事务解耦，不保证 ACID 本地消息表 依赖数据库 可靠消息 无法回滚，需要补偿 总结： 强一致性：2PC &#x2F; 3PC，适用于对事务要求极高的场景。 最终一致性：TCC &#x2F; Saga &#x2F; MQ &#x2F; 本地消息表，适用于高吞吐量或长事务。 TCC &#x2F; Saga 需要开发者手动管理事务，2PC &#x2F; 3PC 由事务管理器自动处理。 选择哪种方案，取决于业务需求、性能要求以及对一致性的容忍度。 &#x2F;&#x2F; 待补充学习 补充：分布式系统的开发中，延迟是个很重要的指标。评估服务可用性–&gt;负载均衡和容灾；评估领导者节点可用性–&gt;是否发起领导选举。 参考资料： 如何选择分布式事务解决方案？ 服务治理：分布式事务解决方案有哪些？ —某天职场老兵William突然抽查我的八股基础，回来赶紧灰溜溜补上……"},{"title":"【技术思考】工程上的最佳实践","path":"/2025/03/23/【技术思考】工程上的最佳实践/","content":"正式进入工作岗位之前对精进技术的思考——工程上的最佳实践 Why？首先要理解为什么要从工程实践的角度思考，常规的培训教程虽然是以项目的形式，但目的是帮助我们学会使用基本的开发工具如何使用，而实际开发过程中如何将各种技术组件有效地组合和应用、如何解决实际的业务问题，则是进一步需要关注的问题。 How？以原有的点评项目为例进行思考，可以考虑各部分设计的原因，能否优化： 消息中间件：思考使用场景，如订单状态更新、用户评价通知等，分析为什么要使用消息中间件，以及如何设计消息的生产、消费和存储。 缓存：考虑缓存的使用场景，如热门商品信息缓存、用户会话缓存等，分析如何识别并处理热key，以及缓存的更新和失效策略。 数据库设计与优化：审视数据库表的设计是否合理，是否符合业务需求，以及如何通过索引优化、查询优化等手段提升数据库性能。 微服务架构：分析拆分是否合理，服务之间的依赖关系是否清晰，服务降级、熔断和负载均衡策略是否有效。 服务上线：如何上线，例如灰度发布中的流量染色，如何保证服务的高可用性。 以微服务为例进行思考，考虑从调用和原理到设计决策的转变： 之前学习时，重点可能放在了如何调用中间件以及它们的底层原理上。 现在需要将重心转移到：在给定业务场景下，为什么要这样设计这些组件。 例如，微服务的负载均衡、服务发现、降级熔断等模块，不仅要清楚它们的作用，更要结合业务场景思考如何拆分微服务，以及制定相应的服务降级、熔断和负载均衡策略。以分布式事务为例，虽然有TCC、二阶段提交等理论方案，但在实际开发中，这些方案的严格实现需要很多条件支持，如数据库的兼容性、业务操作的反向接口等。在实际生产中，接口可能无法完美支持这些理论方案，这时需要考虑其他方法，如对账机制，来保证最终一致性，尤其是在对强一致性要求不高的业务场景中。以对账机制的工程实践为例，可能在实现最终一致性的时间间隔上较长，但其泛用性广且易于实现，设计对账机制需要考虑如何在业务执行频率较低时进行对照，以及如何处理幂等性等问题。 在上述思考过程中，可以借鉴大众点评、美团外卖、饿了么、滴滴等成熟项目的技术文章，了解它们在工程实践中的经验和教训。思路放宽，例如，各大应用基本都有点赞模块，了解它们是如何实现的，筛选出和自己的项目比较贴合的部分深入研究。 例如，大众点评在订单系统分库分表实践中的垂直切分和水平切分策略，以及如何通过Hash切分实现数据的均匀分布和易于扩展的架构。 DDD在大众点评交易系统演进中的应用 2-大众点评内容平台架构实践-三木 大众点评订单系统分库分表实践 了解了实践中的技术原理之后，再进一步关注通用方法论的提炼： 第一个是基础架构平台层面。例如，分布式ID发号器（如Leaf）、热Key检测与治理、大文件分布式对象存储（如JFS）等，理解这些通用实践，为自己的项目提供参考和借鉴，提升解决实际问题的能力。 第二个是思想层面。例如，在无法控制仓库报送数据的情况下，通过数据分析确定需要特殊处理的仓库，采用简单粗暴但有效的硬编码方式解决问题。此处提炼的方法论是：当技术手段难以直接解决问题时，可以通过分析实际数据和业务场景，找到变通的非技术性方法来绕过技术难题。 以上为主线任务，接下来是支线任务，即提前了解部门技术栈和业务后，例如云原生相关可以了解： Kubernetes稳定性保障实践 AutoMQ官方账号 —-与职场老兵William的对话整理"},{"title":"【通用工具】Git分布式版本控制工具","path":"/2025/03/21/【通用工具】Git分布式版本控制工具/","content":"Git有两个基本作用: 版本控制 团队开发 一、Git工作流程 二、Git基本配置设置用户信息设置：（+如果要查看，只输入双引号前面的就好了） 12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; 有3种范围：--local只对某个仓库有效，--global对当前用户的所有仓库有效，--system对系统所有登录的用户有效。 要显示config的配置，加--list。 三、Git基本使用获取本地仓库git init，执行之后工作目录下就会产生.git隐藏目录。 核心操作 clone（克隆）: 从远程仓库中克隆代码到本地仓库 checkout （检出）:从本地仓库中检出一个仓库分支然后进行修订 add（添加）: 在提交前先将代码提交到暂存区 可以接单个文件名，也可以接通配符 commit（提交）: 暂存区 –&gt; 本地仓库。本地仓库中保存修改的各个历史版本 可以接-m后跟注释 fetch (抓取) ： 从远程库，抓取到本地仓库，不进行任何的合并动作，一般操作比较少 pull (拉取) ： 从远程库拉到本地库，自动进行合并(merge)，然后放到到工作区，相当于fetch+merge push（推送） : 修改完成后，需要和团队成员共享代码时，将代码推送到远程仓库 辅助查看与操作 git status：查看修改的状态 git log [option]：查看提交日志，git-log以精简形式查看 git log：以默认形式查看提交日志 git log --all：显示所有分支的提交日志 git log --pretty=oneline：将提交信息显示为一行 git log --abbrev-commit：使得输出的 commitId 更简短 git log --graph：以图的形式显示提交历史，便于查看分支合并情况 git reflog commitID：记录所有操作，可以回滚到任意地方 git reset --hard commitID：版本回退 添加文件到忽略列表：创建 .gitignore 文件，列出要忽略的文件模式 回滚如果在开发过程中，某个需求不需要了，此时分为3种情况讨论： 文件在工作区：执行git checkout file 文件在暂存区：执行git reset HEAD file，让这个文件回到工作区，然后执行1 文件在本地仓库：执行git reset -方式（有3种） hard：工作区、暂存区、本地仓库3个地方保持一致 mixed：让文件保存在工作区 soft：让文件保存在暂存区 四、Git分支核心操作 查看本地分支：git branch 创建本地分支：git branch 分支名 切换分支：git checkout 分支名 可以直接切换到一个不存在的分支（创建并切换）：git checkout -b 分支名 合并分支：git merge 分支名称，一个分支上的提交可以合并到另一个分支 删除分支：不能删除当前分支，只能删除其他分支 git branch -d b1 删除分支时，需要做各种检查 git branch -D b1 不做任何检查，强制删除 解决冲突步骤： 处理文件中冲突的地方 将解决完冲突的文件加入暂存区(add) 提交到仓库(commit) GitFlow master （生产） 分支：线上分支，主分支，中小规模项目作为线上运行的应用对应的分支； develop（开发）分支：是从master创建的分支，一般作为开发部门的主要开发分支，如果没有其他并行开发不同期上线要求，都可以在此版本进行开发，阶段开发完成后，需要是合并到master分支,准备上线； feature/xxxx分支：从develop创建的分支，一般是同期并行开发，但不同期上线时创建的分支，分支上的研发任务完成后合并到develop分支； hotfix/xxxx分支：从master派生的分支，一般作为线上bug修复使用，修复完成后需要合并到master、test、develop分支； 还有一些其他分支，在此不再详述，例如test分支（用于代码测试）、pre分支（预上线分支）等等。 五、Git远程仓库基本命令 对接远程仓库： git remote add &lt;远端名称&gt; &lt;仓库路径&gt; 查看远程仓库： git remote 推送到远程仓库：git push [-f] [--set-upstream] [远端名称 [本地分支名]:[远端分支名]] 如果远程分支名和本地分支名称相同，则可以只写本地分支git push origin master -f 表示强制覆盖 --set-upstream 推送到远端的同时并且建立起和远端分支的关联关系 如果当前分支已经和远端分支关联，则可以省略分支名和远端名 查看本地分支与远程分支的关联关系：git branch -vv 从远程仓库克隆：git clone &lt;仓库路径&gt; [本地目录] 从远程仓库抓取&#x2F;拉取：（如果不指定远端名称和分支名，就抓取所有分支） git fetch [remote name] [branch name]：将仓库里的更新都抓取到本地，不进行合并。 git pull [remote name] [branch name]：将远端仓库的修改拉到本地并自动进行合并，等同于fetch+merge 解决冲突远程分支也是分支，解决冲突的方式和本地相同（看上文）。 需要先拉取远程仓库的提交，经过合并后才能推送到远端分支： 六、进阶命令交互式变基在本地使用，重新排序、合并、拆分、编辑或删除提交，从而整理提交历史，使其更加清晰。 rebase和merge的区别：都是实现合并分支，但是细节不同，rebase会把复杂的提交历史修订为干净整洁的线性结构，并且产生新的commitID。 使用步骤： 执行git rebase -i HEAD~5，此时打开一个编辑器，显示最近的5个提交，每个提交前有一个命令（默认是pick） 编辑提交列表，修改每一行前面的命令，例如pick改成reword Git会按照指令提交 常用命令： 命令 缩写 作用 pick p 保留该提交（不做修改） reword r 修改提交信息 edit e 暂停 rebase，允许修改提交内容（如增删文件）或提交信息 squash s 合并到前一个提交，并保留提交信息 fixup f 合并到前一个提交，但丢弃当前提交的提交信息 drop d 删除该提交 exec x 执行一个 shell 命令（如运行测试） 注意事项： 不要修改已经推送到远程仓库的提交历史（除非确定没有其他人基于此工作） 如果遇到冲突： 解决冲突后，用 git add 标记为已解决 继续 rebase：git rebase --continue 或终止 rebase：git rebase --abort 解决后强制推送到远程：git push --force 储藏 临时保存未提交的更改，将当前工作目录和暂存区的修改保存到一个“储藏区”（stash stack），可以快速切换分支或处理其他任务，后续再恢复这些更改。 基本命令： 基本命令 具体操作 说明 存（入栈） stash (push) 默认存入当前工作区的修改到栈中 stash save &quot;注释&quot; 可以连续存多次变动代码，添加注释方便区分 取（出栈） stash pop 取出栈顶的修改并应用到当前工作区，同时从栈中移除该修改 注意：确保此时 pop 的变动代码是你需要的，否则 pop 后可能需要重新压栈 stash apply 取出栈顶的修改并应用到当前工作区，但不从栈中移除该修改（类似 peek） 清除 stash drop 丢弃栈顶的修改 stash clear 清空整个 stash 栈 查看 stash list 查看 stash 栈中的所有修改记录 stash show + 栈索引 查看指定索引位置的修改详情 使用场景： 切换分支时，当前分支有未完成的代码。 需要紧急修复其他分支的 Bug，但不想提交当前代码。 临时保存实验性代码，避免污染提交历史。 合并冲突前，先保存当前改动。 使用示例： 12345678910111213141516# 1. 当前有未提交的修改，但需要切换到其他分支git stash# 2. 切换到其他分支并完成任务git checkout other-branch# ... 处理其他任务 ...# 3. 返回原分支并恢复储藏git checkout original-branchgit stash pop # 恢复并删除最近的储藏# 4. 查看储藏列表（可选）git stash list# 5. 清空储藏（可选）git stash clear 挑选工作在多分支结构的提交维度上，与merge的区别： merge：需要另一个分支上的所有变动 cherry-pick：需要另一个分支上的部分变动 提交情形： 当产生冲突时，会停下来让用户决定，此时有3种情况： --continue：解决冲突后，git add，再执行此命令继续合并 --abort：放弃合并，回到之前的状态 --quit：放弃合并，且不回到之前 比较不同diff命令，主要讲解两点提交和三点提交的区别： 七、铁令 切换分支前先提交本地的修改 代码及时提交，提交过了就不会丢 遇到任何问题都不要删除文件目录 tag与go modules Git 和 Go Modules 的一个重要机制：Tag（标签）是打在具体的 Commit（提交）上的，而不是打在分支上的。 只要你的那个 Commit (e9680046ed2f) 存在于远程仓库中（无论它是在 feature 分支、develop 分支，还是悬空的），只要你给它打上了 v1.3.21 的标签并推送到远程，Go 就能引用到它。 具体操作流程你只需要确保针对那个 Commit 打了标签并推送即可： 1. 在 go-idt 仓库操作 你甚至不需要切换分支，直接指定 Commit ID 打标签即可： 12345# 直接给指定的 commit hash 打标签 (无论它在哪个分支)git tag v1.3.21 e9680046ed2f# 单独推送这个标签到远程git push origin v1.3.21 2. 在引用方（你的主项目）操作 123# 修改 go.mod 中的 replace 为 v1.3.21# 然后执行go mod tidy 为什么这样做是可以的？Go 的依赖管理工具（go mod）在拉取代码时，逻辑是这样的： 去找 code.sangfor.org/.../go-idt.git 这个仓库。 询问仓库：“有没有一个叫 v1.3.21 的标签？” 仓库回答：“有，这个标签指向 Commit e9680046ed2f。” Go 下载那个 Commit 的代码。 它完全不关心这个 Commit 是属于 master 分支还是某个临时的开发分支。 只有一个小注意事项虽然技术上不需要合并到 master，但在管理上建议注意： 如果你的 v1.3.21 是基于一个临时分支打的，以后这个分支被删除了，或者这个 Commit 永远没合入主线，可能会导致代码历史比较混乱（虽然 Tag 依然存在，代码不会丢，但在 git log 的树形图里看会比较孤立）。 修改分支的base：ontogit rebase --onto 是 Git 中最强大的命令之一，你可以把它理解为 “精准的外科手术式移植”。 普通的 git rebase master 只能处理简单的“把当前分支基底换到 master”的情况。而 --onto 允许你截取某一段特定的提交记录，嫁接到另一个地方去。 它主要用于解决 “链式分支依赖”（Branch Chaining）的问题。 核心语法1git rebase --onto &lt;移植到的目标位置&gt; &lt;被剪切的起始位置(不包含)&gt; &lt;要移动的分支&gt; 也就是： git rebase –onto 最经典的场景：孙子分支“借尸还魂”假设你有以下分支结构： master: 主干。 feat-A: 基于 master 开发的功能 A。 feat-B: 基于 feat-A 开发的功能 B（因为 B 依赖 A 的代码）。 现在，feat-A 因为某些原因被废弃了（或者已经合并进 master 了，或者你想把 feat-B 独立出来），你需要把 feat-B 单独移到 master 上，并且不要 feat-A 的那些提交。 1. 现状图示Plaintext 12345A --- B (master) \\ C --- D (feat-A) \\ E --- F (feat-B) 如果你直接在 feat-B 上执行 git rebase master，Git 默认会沿着线往回找，把 C, D, E, F 全部拿过来。这通常不是你想要的，你不想要 C 和 D。 2. 目标图示你想达到的效果是把 E 和 F 摘出来，直接接在 B 后面： Plaintext 1234A --- B (master) | \\ E&#x27; --- F&#x27; (feat-B) (注意：C 和 D 依然在 feat-A 上，但 feat-B 已经和它们没关系了) 3. 使用 git rebase --onto你需要告诉 Git 三件事： 嫁接到哪？ master 从哪剪断？ feat-A (意思是从 feat-A 之后开始算) 移动谁？ feat-B 命令如下： Bash 1git rebase --onto master feat-A feat-B 翻译成人话就是： “请把 feat-B 分支上，排除掉 feat-A 里的那些提交，剩下的部分（也就是 E 和 F），移植到 master 上去。” 另一个常用场景：删除中间的一段提交虽然可以用 git rebase -i (交互式) 来做，但 --onto 也可以用来快速剔除一段提交。 假设你的提交历史是： base — commit1 — commit2(坏的) — commit3(坏的) — commit4(好的) — HEAD 你想删掉 commit2 和 commit3，直接让 commit4 接在 commit1 后面。 Bash 123# 这里的 base 指的是 commit1 的哈希值# 这里的 bad-commit-end 指的是 commit3 的哈希值git rebase --onto &lt;commit1_hash&gt; &lt;commit3_hash&gt; current_branch 这会将 commit3 之后 的所有提交（也就是 commit4），嫁接到 commit1 上。 总结与记忆法记住这三个参数的位置： 1git rebase --onto 目的地 剪切点(不含) 当前分支 参数 1 (Target): 地基。新的树根。 参数 2 (Upstream): 以前的旧树根（你想从哪里把树枝砍断）。Git 会忽略这个点以及它之前的所有 commit。 参数 3 (Branch): 树枝的顶端。 风险提示执行这个命令后，可能会遇到**冲突 (Conflict)**。因为你跳过了中间的某些提交（比如上面例子中的 C 和 D），如果 E 和 F 依赖于 C 或 D 修改过的代码，Git 就不知道该怎么合并了，这时你需要手动解决冲突。 使用举例之前在local开发分支，这个分支大家都在往里合代码，然后我现在的代码一直在rebase这个开发分支，但是后来计划有变，需要从开发分支的某个提交拉取一个新的开发分支，然后我的代码需要合并到这个新的开发分支上： 1git rebase --onto origin/develop-&lt;新开发分支&gt; origin/develop-&lt;老开发分支&gt; &lt;特性分支名&gt; 仓库推送标准流程就是：先提交代码入库，再执行命令发布。 为了保证公司内部引用的版本规范和安全，建议你严格按照以下 4 个步骤操作： 1. 修改版本号 (Version Bump)在提交代码前，非常重要的一步是修改版本号。 通常是在 setup.py、pyproject.toml 或 init.py 里。 比如把 1.0.1 改成 1.0.2。 原因：如果不改版本号，同事的 pip 可能会认为没有更新，拉不到你的新代码。 2. 提交代码 (Git Commit &amp; Push)把修改后的代码（包含新的版本号）推送到 Git 仓库。 123git add .git commit -m &quot;fix: 修复了某某bug, 版本升级到 1.0.2&quot;git push origin master 这一步是为了存档，确保“代码库”里有这份代码。 3. 打包构建 (Build)通常你需要把源码打包成 Python 的安装包（.whl 或 .tar.gz）。 在项目根目录下执行（示例）： 1python setup.py sdist bdist_wheel 执行完后，你的 dist/ 目录下会生成类似 my-library-1.0.2.tar.gz 或 my_library-1.0.2-py3-none-any.whl 的文件。 4. 上传发布 (Rsync)最后一步，才是用那个 rsync 命令，把刚才打好的包，传到公司的私有源服务器上。 总结顺序不能乱： 改版本号 (否则发了也是白发) Git Push (保存源码) Build (生成安装包) Rsync (上架给别人用)"},{"title":"【Go基础】环境搭建与开发","path":"/2025/03/19/【Go基础】环境搭建与开发/","content":"环境第1步:下载go（下载地址） 第2步：配置环境变量 GOROOT：go的安装目录 GOPATH：go的工作目录（全局），一般给文件夹起名叫GoWorkstation、Go_WorkSpace等。 src：存放源代码 pkg：存放依赖包 bin：存放可执行文件 GOPATH 是 Go 早期（Go 1.11 之前）管理依赖和项目代码的核心环境变量，早期go build、go run或go install等命令会按照当前目录-&gt;绝对路径-&gt;GOPATH路径查找目标代码。从Go 1.11开始，官方推荐使用Go Modules替代 GOPATH，可以在任意目录管理项目，依赖存储在 go.mod 和 go.sum，而非 GOPATH。 其他常用环境变量：GOOS，GOARCH，GOPROXY，国内用户建议设置 goproxy：export GOPROXY=https://goproxy.cn 开发推荐用Goland进行开发，实际运行的时候，如果是写的单独文件，设置按照file文件运行，否则包管理可能会有问题；如果是项目，需要指定目录为xxx/src，指定输出目录为xxx/bin。 Go Modules1. 创建Go Modules项目go mod init calc-mod：在当前目录初始化一个 Go Modules 项目。会创建 go.mod 文件，内容为module calc-mod，calc-mod 是自定义的模块名（通常用于本地开发）。 如果项目计划开源到 GitHub，模块名应改为仓库路径：go mod init github.com/fuxing-repo/fuxing-module-name，通常对应代码仓库的 URL（如 github.com/用户/仓库）。 生成的 go.mod： 1module github.com/fuxing-repo/fuxing-module-name 与 Maven&#x2F;Gradle 的区别：Go 的设计哲学是去中心化，直接通过代码仓库（GitHub&#x2F;GitLab 等）分发模块，依赖 Git Tag 版本化。 无需发布到中央仓库：Go 依赖直接从代码仓库（如 GitHub）下载。 版本控制：通过 Git 的标签（Tag）标记版本（如 v1.0.0）。如需指定版本运行 go get github.com/foo/bar@v1.2.3，Go 会自动更新 go.mod。 2. 完整流程示例步骤 1：初始化模块 1go mod init github.com/fuxing-repo/calculator 步骤 2：编写代码并导入依赖 在 main.go 中导入第三方库（如 github.com/gin-gonic/gin）： 12345678package mainimport &quot;github.com/gin-gonic/gin&quot;func main() &#123; r := gin.Default() r.Run()&#125; 步骤 3：自动下载依赖 运行任意 Go 命令（如 go build 或 go list）： 1go list Go 会： 解析 import 语句，发现依赖 github.com/gin-gonic/gin。 下载最新版本（或符合 go.mod 约束的版本）。 更新 go.mod 和 go.sum 文件。 包的管理Go的import是包级别的，包名就是当前文件夹名称。 一个项目中，可以存在一样的包名，如果需要引用同样的包名，可以用alisa区分。 可见性：无论是变量、函数还是类属性及方法，它们的可见性都是与包相关联的。如果属性名或方法名首字母大写，则可以在其他包中直接访问这些属性和方法，否则只能在包内访问。 结合import可以用一个包名来点出函数、结构体、接口等调用。 入口的package必须是main，否则可以编译成功，但是跑不起来。"},{"title":"【Go基础】错误处理","path":"/2025/03/19/【Go基础】错误处理/","content":"基本认识 在Go中，将错误当成值来进行处理，强调判断错误和处理错误，不支持try/catch捕获异常。 Go选择使用Error而非Exception来进行错误处理。 一般把错误作为函数或方法的最后一个返回值。 Error接口使用error接口表示错误类型。该接口只有一个Error()方法，返回描述错误信息的字符串。 123type error interface &#123; Error() string&#125; 接口类型的默认零值为nil，所以通常把调用函数时返回的错误和nil比较： 12345_, err := someFunc(some parameters)if err != nil&#123; fmt.Println(&quot;出现错误：&quot;, err) // 使用标准库fmt打印错误自动调用error类型的Error方法，打印错误描述信息 return&#125; Go这种机制的好处是，遇到error需要立即处理，而Java中是try/catch中包裹了一大堆代码，良性和致命的问题都会抛出错误，不容易排查问题。 创建错误由于error是接口，可以自定义错误类型（开发中间件使用较多）。 最简单的创建错误的方法是用errors包提供的New函数创建一个错误： 123func New(text string) error&#123; return &amp;errorString&#123;text&#125;\t// 返回一个指针，使得每次返回都是一个新的对象，否则在做等值判断时可能会出问题。&#125; 错误的两种类型error：可以被处理的错误；panic：非常严重不可恢复的错误。 errors包当需要传入格式化的错误描述信息，用fmt.Errorf更好，但是它提供很多描述错误的文本信息，会丢失原本的错误类型，导致错误在做等值判断时失效。为了解决这个缺陷，fmt.Errorf在1.13版本提供了特殊的格式化动词w%，可以基于已有错误再包装得到新的错误： 1fmt.Errorf(&quot;查询数据库失败，err:%w&quot;, err) 对于这种二次包装的错误，errors包提供了4个常用的方法： New：创建一个新的 error func Is(err, target error) bool ：判断err是否包含target，是不是特定的某个error func As(err error, target interface&#123;&#125;) bool：判断error是否为target类型，类型转换为特定的error（用得不多） func Unwrap(err error) error：获得error包含下一层错误，解除包装并返回被包装的 error 使用举例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport (\t&quot;errors&quot;\t&quot;fmt&quot;)func main() &#123;\tvar err error = &amp;MyError&#123;&#125;\tprintln(err.Error())\tErrorsPkg()&#125;type MyError struct &#123;&#125;func (m *MyError) Error() string &#123;\treturn &quot;Hello, it&#x27;s my error&quot;&#125;func ErrorsPkg() &#123;\terr := &amp;MyError&#123;&#125;\t// 使用 %w 占位符，返回的是一个新错误\t// wrappedErr 是一个新类型，fmt.wrapError\twrappedErr := fmt.Errorf(&quot;this is an wrapped error %w&quot;, err)\t// 再解出来\tif err == errors.Unwrap(wrappedErr) &#123; fmt.Println(&quot;unwrapped&quot;)\t&#125; if errors.Is(wrappedErr, err) &#123; // 虽然被包了一下，但是 Is 会逐层解除包装，判断是不是该错误 fmt.Println(&quot;wrapped is err&quot;)\t&#125;\tcopyErr := &amp;MyError&#123;&#125;\t// 这里尝试将 wrappedErr转换为 MyError\t// 注意我们使用了两次的取地址符号\tif errors.As(wrappedErr, &amp;copyErr) &#123; fmt.Println(&quot;convert error&quot;)\t&#125;&#125; panic意味着fatal error，调用者不能解决，彻底结束。可能遇到的场景： 调用别人的代码，别人没有合理使用panic（自己写代码还是用error）。 数组越界、不可恢复的环境、栈溢出等错误。 从panic中恢复： ​\trecover可以进行兜底，把这一次的request放弃，go的runtime会退出，可以去执行其他的request，但是风险比较大，revover一般就是记录个日志之类的，，示例： 123456789101112131415package mainimport &quot;fmt&quot;func main() &#123;\tdefer func() &#123; if data := recover(); data != nil &#123; fmt.Printf(&quot;hello, panic: %v &quot;, data) &#125; fmt.Println(&quot;恢复之后从这里继续执行&quot;)\t&#125;()\tpanic(&quot;Boom&quot;)\tfmt.Println(&quot;这里将不会执行下来&quot;)&#125; 使用原则 遇事不决选 error 当怀疑可以用 error 的时候，就说明不需要 panic 一般情况下，只有快速失败的过程，才会考虑panic defer用于在方法返回之前执行某些动作（类似于Java中的finally），一般用来释放资源（如锁等）。执行顺序：像栈一样，先进后出。 处理错误正常流程的代码推荐的写法，err处理缩进，正常的代码是一条直线。 12345678910111213/////////推荐写法////////f, err := os.Open(path)if err != nil &#123; // handle error&#125;// do stuff//////////不推荐///////f, err := os.Open(path)if err == nil &#123; // do stuff&#125;// handle error 少写if err !&#x3D; nil的技巧 返回err或者nil，可以直接return： 12345678910111213//////原来的写法func AuthenticateRequest(r *Request) error &#123; err := authenticate(r.User) if err != nil &#123; return err &#125; return nil&#125;//////推荐的写法 //毕竟函数的返回值就要error类型，而且调用函数之后就返回一个error类型，那直接return就好了func AuthenticateRequest(r *Request) error &#123; return authenticate(r.User)&#125; 用io.Reader统计读取内容的行数 1234567891011121314151617181920212223242526272829//////原来的写法func CountLines(r io.Reader) (int, error) &#123; var ( br = bufio.NewReader(r) lines int err error ) for &#123; _, err = br.ReadString(&#x27; &#x27;) lines++ if err != nil &#123; break &#125; &#125; if err != io.EOF &#123; return 0, err &#125; return lines, nil&#125;//////推荐的写法 //func CountLines(r io.Reader) (int, error) &#123; sc := bufio.NewScanner(r) lines := 0 for sc.Scan() &#123; lines++ &#125; return lines, sc.Err()&#125; 利用bufio.Scanner方法，这个里面封装了按行读取的逻辑，并且其Scan方法读取时遇到错误会记录下来，最终通过 sc.Err()统一返回。 包装错误类型，缓存错误 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758////////////原来的写法type Header struct &#123; Key, Value string&#125;type Status struct &#123; Code int Reason string&#125;func WriteResponse(w io.Writer, st Status, headers []Header, body io.Reader) error &#123; _, err := fmt.Fprintf(w, &quot;HTTP/1.1 %d %s\\r &quot;, st.Code, st.Reason) if err != nil &#123; return err &#125; for _, h := range headers &#123; _, err := fmt.Fprintf(w, &quot;%s: %s\\r &quot;, h.Key, h.Value) if err != nil &#123; return err &#125; &#125; if _, err := fmt.Fprintf(w, &quot;\\r &quot;); err != nil &#123; return err &#125; _, err = io.Copy(w, body) return err&#125;//////////////推荐的写法 //type errWriter struct &#123; io.Writer err error\t// 用来暂存&#125;func (e *errWriter) Write(buf []byte) (int, error) &#123; if e.err != nil &#123; return 0, e.err &#125; var n int n, e.err = e.Writer.Write(buf) return n, e.err&#125;func WriteResponse(w io.Writer, st Status, headers []Header, body io.Reader) error &#123; ew := &amp;errWriter&#123;Writer: w&#125; fmt.Fprintf(ew, &quot;HTTP/1.1 %d %s\\r &quot;, st.Code, st.Reason) for _, h := range headers &#123; fmt.Fprintf(ew, &quot;%s: %s\\r &quot;, h.Key, h.Value) &#125; fmt.Fprintf(ew, &quot;\\r &quot;) io.Copy(ew, body) return ew.err&#125; 下面这种写法，不用做任何err的判定，相当于在包装类里面复用了，更优雅。 使用errors包装错误-从根本上解决上一小节只是减少了if err !&#x3D; nil的数量，但是并没有从根本上解决不能提供详细上下文的问题。一方面，破坏原始错误，担心上层调用的人用做等值判定，只能一层层向上透传，最终输出没有堆栈没有上下文的信息，令人崩溃；另一方面，又想包装更多详细错误信息。 error.Wrap()：保留原始错误信息，捎带一些附加信息。 errors.Cause()：用来获取原始错误（根因，root error）。 errors.WithMessage()：不保存堆栈信息。 实际应用时： 自己的应用代码中，使用errors.New()或者errors.Errorf()返回错误； 如果调用其他包内的函数，直接返回，往上抛，不要在错误的地方到处打日志。（满足原则：只处理一次。） 如果使用三方库&#x2F;标准库，使用errors.Wrap()或errors.Wrapf()保存堆栈信息。 程序的顶部或者工作的goroutine顶部，用%+v详细记录堆栈。 处理错误的原则处理的原则是：如果遇到错误，只处理一次。 一些经常出现的错误代码，在错误处理中，既记录了日志，又返回了错误： 12345678func WriteAll(w io.Writer, buf []byte) error &#123; _, err := w.Write(buf) if err != nil &#123; log.Println(&quot;unable to write:&quot;, err) return err &#125; return nil&#125; 这个时候又尬住了，一方面，不记录日志，找不到是谁报错；另一方面，记录日志接下来调用者层层打印，在控制台的输出可能就层层割裂，没有完整的堆栈信息。 继续讲处理的原则：错误处理契约规定，出现错误时，不能对其他返回值的内容做任何假设。如果程序员忘记return，函数返回的结果可能是正确的，但是其他返回值的内容是错误的。 那么应该如何记录日志？原则： 错误要被日志记录 *应用程序处理错误，保证**100%*完整性 之后不再报告当前错误 结合上一小节，包装错误的原则： 如果你提供的库很多人使用，不应该使用任何wrap包装错误 如果你的函数无法处理错误，携带足够多的上下文，用wrap.Errors往上抛（足够的上下文：能帮助解决问题，一般是什么人调用了什么接口，返回成功还是失败） 如果这个错误被处理过，就不要再抛了。 【参考资料】 https://www.liwenzhou.com/posts/Go/error/"},{"title":"【Go基础】并发编程基本概念","path":"/2025/03/19/【Go基础】并发编程基本概念/","content":"并发编程基本概念串行、并发与并行 串行：我们都是先读小学，小学毕业后再读初中，读完初中再读高中。 并发：同一时间段内执行多个任务（你在用微信和两个女朋友聊天）。 并行：同一时刻执行多个任务（你和你朋友都在用微信和女朋友聊天）。 进程、线程和协程 进程（process）：程序在操作系统中的一次执行过程，系统进行资源分配和调度的一个独立单位。 线程（thread）：操作系统基于进程开启的轻量级进程，是操作系统调度执行的最小单位。 协程（coroutine）：非操作系统提供而是由用户自行创建和控制的用户态‘线程’，比线程更轻量级。 并发模型4种常见的实现方式： 线程&amp;锁模型 Actor模型 CSP模型（communicating sequential processes，Go中主要是基于CSP的goroutine和channel实现） Fork&amp;Join模型"},{"title":"【Go基础】垃圾回收演进 三色标记法","path":"/2025/03/19/【Go基础】垃圾回收演进三色标记法/","content":"GO1.3标记清除，整体需要STW：1.暂停，找到可达和不可达对象，2. 标记可达对象，3. 清除未标记对象，4. 结束暂停 GO1.5三色标记法，堆启动写屏障，栈不启动，全部扫描一次后，需要重新扫描栈（STW），效率低 如果没有STW，对象丢失的2个条件： 黑色对象指向白色对象（白色挂在黑色下面） 灰色对象与其可达白色对象之间遭到破坏（灰色也丢失了该白色） 屏障机制，保障对象不丢失的2种方式： 强三色不变式：不允许黑色对象指向白色对象 弱三色不变式：允许黑色对象指向白色对象，但是该白色对象要被灰色对象可达 为此，go初步得到两种屏障方式： 插入写屏障：只使用在堆中，将黑色指向的白色对象标记为灰色；栈要启动STW重新三色标记扫描（仍然需要STW重新扫描栈） 删除写屏障：被删除的白色节点标记为灰色（保护灰色对象到白色对象的路径不会断），所以最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉（回收精度低，开始时STW记录初始快照） 为什么在栈中不使用： GO1.8三色标记法，混合写屏障，栈不启动，堆启动，几乎不需要STW，效率高 结合得到混合写屏障（满足弱三色不变）： 开始时将所有栈上的可达节点标记为黑色，在GC期间栈上新增的也标记为黑色（无需STW） 删除和新增的全部标记为灰色 参考资料： https://www.bilibili.com/video/BV1wz4y1y7Kd/"},{"title":"【Go基础】Go入门与实践资源帖","path":"/2025/03/19/【Go基础】Go入门与实践资源帖/","content":"看到好的持续更新…… Go系统教程 从语法讲起：李文周博客 七天快速上手项目 Go测试驱动开发博客 孔令飞项目开发实战课程，孔令飞图文教程 《Go 语言高级编程》书籍 Go算法刷题模板 Go实战项目 KV系统 crawlab分布式爬虫平台 seaweedfs分布式文件系统 Cloudreve云盘系统 gfast后台管理系统（基于Go Frame） alist多存储文件列表（基于Gin、React） Yearning开源SQL审核平台 Go要点 GMP机制 并发编程机制 编辑解释运行机制 GC机制 看过的帖子 腾讯技术工程：协程调度的本质 通用技术面试相关 极客时间面试指导视频课"},{"title":"【Go基础】微服务概念与演进","path":"/2025/03/19/【Go基础】微服务概念与演进/","content":"微服务概念与演进巨石架构到微服务的演进传统网页应用虽然进行了模块化设计，但是最终仍然是打包成一个war包进行部署，启动慢，无法拓展，可靠性很低。 什么是微服务是面向服务的架构模式（SOA）的最佳实践。定义：围绕业务功能构建的，服务关注单一业务，服务间采用轻量级的通信机制，可以全自动独立部署，可以使用不同的编程语言和数据存储技术。微服务架构通过业务拆分实现服务组件化，通过组件组合快速开发系统，业务单一的服务组件又可以独立部署，使得整个系统变得清晰灵活。 如何实现微服务组件服务化，例如在Go中实现需要：• kit：一个微服务的基础库（框架）• service：业务代码 + kit 依赖 + 第三方依赖组成的业务微服务• RPC + message queue：轻量级通讯按业务组织服务，常见的模式：大前端（移动&#x2F;Web） &gt; 网关接入 &gt; 业务服务 &gt; 平台服务 &gt; 基础设施（PaaS&#x2F;Saas） 微服务的优点和缺点优点：原子服务、独立进程、隔离部署、去中心化服务治理。 缺点：基础设施的建设、复杂度高。固有的复杂性：① 必须使用RPC或者消息传递实现进程间的通信，处理通信慢与局部失效的问题；② 不同服务可能使用不同数据库。 要搞定这些缺点，需要做怎样的基础设施建设做什么事情–可用性设计、API设计等引申话题 微服务组件微服务设计中常见的角色三大组件：API Gateway、BFF层、底层服务Microservices。API网关分层： 流量入口：协议转换&#x2F;路由分发 安全边界：统一认证&#x2F;权限控制 流量治理：限流熔断&#x2F;日志监控 BFF适配层：为不同终端提供定制化APICQRS模式：读写分离（通过binlog实现数据同步） 演进过程Microservices拆分: 按垂直功能性能角度，含久必分分久必合 可以考虑中间加一层(例如统一接入账号) 按照业务领域抽象（DDD） 按照功能拆分：CQRS，应用程序分为命令端和查询端。 安全问题外部的安全保障：API Gateway统一认证拦截 -&gt; BFF校验Token -&gt; Service 服务内部的安全保障：① 认证：知道是谁调用的；② 授权：RBAC，控制能访问哪些接口。 信任等级：Full&#x2F;Half&#x2F;Zero Trust。 gRPC概念A high-performance, open-source universal RPC framework. 高性能开源框架。特性： 支持多语言； 序列化支持Protocol Buffer和Json； HTTP&#x2F;2多路复用。 其中最重要的是标准健康监测协议，应用如下：应用1：服务稳定与不稳定时摘除与恢复；应用2：外挂容器健康检测；应用3：生产者与消费者之间的检测；应用4：平滑发布。 服务发现的模型① 客户端发现（微服务的核心是去中心化，用这种更好）；② 服务端发现（如果服务很大，可能用service mesh）。 多集群与多租户多集群why?——单集群坏处：一般布N+2来冗余节点。出现故障时后果严重。how?——物理上两套资源，逻辑上维护cluster概念。好处？——不同集群使用多套独占的缓存，性能好。坏处？——缓存命中率下降，不同业务形态数据正交。拓展：从全集群中选取一批节点（子集），利用划分子集限制连接池大小。——子集算法。 多租户why？保障代码隔离性，基于流量租户做路由决策。问题：[并行测试]时混用环境不可靠，而多布环境成本高，也难以做压测。解决方法：染色发布，基于流量类型做路由。本质是从源头传递一个标签，挂在go的上下文中，基于RPC负载均衡的流量做路由,路由到指定的节点。 参考资料：https://microservices.io/index.htmlhttps://blog.csdn.net/mindfloating/article/details/51221780https://www.cnblogs.com/dadadechengzi/p/9373069.htmlhttps://www.cnblogs.com/viaiu/archive/2018/11/24/10011376.htmlhttps://www.cnblogs.com/lfs2640666960/p/9543096.htmlhttps://mp.weixin.qq.com/s/L6OKJK1ev1FyVDu03CQ0OAhttps://www.bookstack.cn/read/API-design-guide/API-design-guide-02-面向资源的设计.mdhttps://www.programmableweb.com/news/how-to-design-great-apis-api-first-design-and-raml/how-to/2015/07/10http://www.dockone.io/article/394https://www.jianshu.com/p/3c7a0e81451ahttps://www.jianshu.com/p/6e539caf662dhttps://my.oschina.net/CraneHe/blog/70317https://my.oschina.net/CraneHe/blog/703169https://my.oschina.net/CraneHe/blog/703160 学习笔记，侵删。"},{"title":"【LeeCode】刷题记录.md","path":"/2024/04/08/【LeeCode】刷题记录/","content":"作者：力扣官方题解 来源：力扣（LeetCode） LeeCode热题10049、字母异位词分组（中）https://leetcode.cn/problems/group-anagrams/solutions/520469/zi-mu-yi-wei-ci-fen-zu-by-leetcode-solut-gyoc/ 题面给你一个字符串数组，请你将 字母异位词 组合在一起。可以按任意顺序返回结果列表。 字母异位词 是由重新排列源单词的所有字母得到的一个新单词。 为什么是哈希表相关的题？ 思路： 当把单词中所有字母按照字母顺序表排列时，字母异位词的排序后的单词是相同的。 可以使用相同点作为一组字母异位词的标志，使用哈希表存储每一组字母异位词。 哈希表的键为一组字母异位词的标志，哈希表的值为一组字母异位词列表。 具体做法：遍历每个字符串，对于每个字符串，得到该字符串所在的一组字母异位词的标志，将当前字符串加入该组字母异位词的列表中。遍历全部字符串之后，哈希表中的每个键值对即为一组字母异位词。 方法1：字母排序 构造单词的字符排序，作为键。 将单词加入散列表。 返回答案。 1234567891011121314class Solution &#123; public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) &#123; Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;String, List&lt;String&gt;&gt;(); for (String str : strs) &#123; char[] array = str.toCharArray(); Arrays.sort(array); String key = new String(array); List&lt;String&gt; list = map.getOrDefault(key, new ArrayList&lt;String&gt;()); list.add(str); map.put(key, list); &#125; return new ArrayList&lt;List&lt;String&gt;&gt;(map.values()); &#125;&#125; 复习 char[] toCharArray() 。将此字符串转换为新的字符数组。 getOrDefault。HashMap的一个方法，返回指定键映射到的值，如果此映射不包含键的映射，则返回 defaultValue 。 向list中新增元素用add方法。 向哈希表中新增元素用put方法，同时传入键和值。 复杂度分析 方法2：计数 互为字母异位词的两个字符串包含的字母相同，因此两个字符串中的相同字母出现的次数一定是相同的，故可以将每个字母出现的次数使用字符串表示，作为哈希表的键。 字符串只包含小写字母，因此对于每个字符串，可以使用长度为 26 的数组记录每个字母出现的次数。 首先统计字符的出现顺序，然后构造键，把具有相通特征的字符串的单词们放在一组，最后返回结果。 12345678910111213141516171819202122232425class Solution &#123; public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) &#123; Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;String, List&lt;String&gt;&gt;(); for (String str : strs) &#123; int[] counts = new int[26]; int length = str.length(); for (int i = 0; i &lt; length; i++) &#123; counts[str.charAt(i) - &#x27;a&#x27;]++; &#125; // 将每个出现次数大于 0 的字母和出现次数按顺序拼接成字符串，作为哈希表的键 StringBuffer sb = new StringBuffer(); //可变字符串对象 for (int i = 0; i &lt; 26; i++) &#123; if (counts[i] != 0) &#123; sb.append((char) (&#x27;a&#x27; + i)); sb.append(counts[i]); &#125; &#125; String key = sb.toString(); List&lt;String&gt; list = map.getOrDefault(key, new ArrayList&lt;String&gt;()); list.add(str); map.put(key, list); &#125; return new ArrayList&lt;List&lt;String&gt;&gt;(map.values()); &#125;&#125; 复杂度分析 面试要点 通过分析，能否意识到单词和键的映射关系。 利用散列表高效储存结果。 数据结构和常见的库函数。"},{"title":"【资源帖】学习Java和算法","path":"/2024/04/06/【资源帖】学习Java和算法/","content":"Java教程学习路线【黑马程序员】 Java简版基础教程：https://www.bilibili.com/video/BV1Cv411372m/ 书：《Java核心技术 1》 书：《Head First Java》 Java Web框架：https://www.bilibili.com/video/BV1m84y1w7Tb/ 单体项目开发： 苍穹外卖：https://www.bilibili.com/video/BV1TP411v7v6/ 微服务： 全套微服务技术栈：https://www.bilibili.com/video/BV1S142197x7/ 企业级项目实战（选择学习）： 学成在线【在线教育】：https://www.bilibili.com/video/BV1j8411N7Bm/ 黑马头条【企业级微服务项目】 ：https://www.bilibili.com/video/BV1Qs4y1v7x4/ 面试专题： 2023版：https://www.bilibili.com/video/BV1yT411H7YK/ 复制标题和时长：https://www.bilibili.com/read/cv22846057/ 精进指南 （JavaWeb后，选学）MySQL：https://www.bilibili.com/video/BV1Kr4y1i7ru/ （微服务后，选学）Redis微服务：https://www.bilibili.com/video/BV1cr4y1671t/ （微服务后，选学）MybatisPlus：https://www.bilibili.com/video/BV1Xu411A7tL/ （JVM前，必学）计算机网络：https://www.bilibili.com/video/BV1c4411d7jb/ （Java基础，选学）JVM虚拟机：https://www.bilibili.com/video/BV1r94y1b7eS/ （JVM后，选学）并发编程：https://www.bilibili.com/video/BV16J411h7Rd/ （git版本控制，选学）https://www.bilibili.com/video/BV1MU4y1Y7h5/ 资源帖 JavaGuide：JavaGuide（Java学习&amp;面试指南） | JavaGuide JavaBooks：https://gitee.com/itwanger/JavaBooks 拿个offer：拿个offer - 开源&amp;项目实战 (nageoffer.com) LeeCode刷题指南官网力扣 (LeetCode) 全球极客挚爱的技术成长平台 刷题指北peach买个共享会员账号看考察频次。 资源帖 labuladong：本站简介 | labuladong 的算法笔记（提升算法能力。） 代码随想录：代码随想录 (programmercarl.com)（全面，但是精简，适合面试突击。） 小林coding：小林coding (xiaolincoding.com)（图解好理解，但只有Redis、MySQL、计网。） 左程云：左程云的个人空间-左程云个人主页-哔哩哔哩视频 (bilibili.com)（算法讲解。） NeeCode：NeetCode（英文站点。） 推荐书单 《剑指offer》。"},{"title":"【求职】如何写一份受欢迎的校招简历","path":"/2024/04/05/【求职】如何写一份受欢迎的校招简历/","content":"常见问题 过度包装设计。减弱主要信息能量，华而不实。 篇幅过长。 求职定位不明。 实践经历描述不当。 一份简历闯天下。 JD：工作职责、工作胜任力。 使用表格式简历。 啰啰嗦嗦重点不突出。 不该讲的乱讲。例如，创业、离职原因、到岗时间、离婚、错误检讨、薪资条件。 优秀简历的特征版面设计简洁大方、布局清晰、模板分界。 简历结构结构完整、详略得当、易于阅读。 内容呈现逻辑清晰、优势突出、数据支撑。 人岗匹配有的放矢、贴近岗位JD、天生我才。 效果 脱颖而出、入得法眼。 顺畅读完，越读越喜欢。 打动人心，不如见一面。 为面试好印象做好铺垫。（面试官其实是根据初印象，步步求证是否确实是需要的人。） 简历的完整结构“2+2”通用的（非本专业&#x2F;技术岗）： 基本信息：略写。7%。 自我评价：较详。20%。 工作经历：详写。（大力气。）70%。 学历、证书、技能：略写。3%。 基本信息 姓名+求职意向+性别+年龄。（政治面貌：国企央企等写，外企不写，民企无所谓。） 联系方式：城市、电话、微信、邮箱。（不用写太多。城市可以写XX(意向城市)。联系方式三个必有一，推荐电话。） 个人照片：彩色、正面头像、有精气神。（匹配行业。） 自我评价&#x2F;教育背景社招： 工作背景。例如，年份+领域&#x2F;行业+擅长&#x2F;熟悉&#x2F;掌握。 优势能力。四条分号隔开。专业软件可以写。 职业素养。 校招： 起止时段：学校、专业、学历、学位。 主修课程。 奖学金可以写。 工作经历&#x2F;实习经历&#x2F;项目经历社招： 工作时段。（可以有总分，总的在某个公司，分的是不同岗位。最好是倒叙。） 工作职责。（前3-5个。） 工作业绩。（为了醒目，可以换个标志，比如五角星。一定要有数据，没数据也不要乱讲。） 工作获奖。（要有含金量的，行业、省市级以上，发明专利等。） 校招： 起止时段、公司、岗位。 工作职责、价值、奖项。 其他佐证 学历背景：学校、专业。 语言能力：语种、级别。（只是针对某些需要语言能力的岗位。其他：听说读写能力流利，可作为工作语言。） 专业技能：证书、级别。（例如岗位资格证。） 校招和社招的区别 教育背景前置&#x2F;后置。 自我评价的有无。（复盘能力。） 优秀简历写作心法人岗匹配！！！ 职场的本质是价值交换。（以终为始。） 见字如面，格式细节很重要，大小标题和逻辑关系。 凤头猪肚豹尾。自我评价漂亮客观，工作经历饱满有结果，其他佐证简短有力。 工作经历倒叙。写清楚总分，闭环表达，数据支撑。 工作年限5年以下，请用一张A4纸完成。 如果经历比较少，根据一段经历可以多挖掘，例如，“1+3+6+x”主轴。 如何准备 确定自己身份：校招&#x2F;社招，确定目标岗位，了解岗位JD。 准备模板，通读三遍。准备素材，多多益善。（所有经历都可以准备。） 现有骨架，再填充。时间倒叙，先粗后细。 先写草稿，反复打磨。"}]